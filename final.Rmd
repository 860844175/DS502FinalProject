---
title: "House Pricing Prediction"
subtitle: "DS502 Final Project"
author: "Yufei Lin, Jingfeng Xia, Jinhong Yu, Shijing Yang, Yanze Wang"
date: "Nov 29 2020"
output:
  pdf_document: default
  word_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
# check R version
R.Version()$major

# set up document
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(fig.pos = "H", out.extra = "")
knitr::opts_chunk$set(fig.width=5,fig.height=3)
library(pander)
library(knitr)
library(skimr)
library(kableExtra)
library(tinytex)
library(dplyr)
library(purrr)
local({
  hook_inline = knitr::knit_hooks$get('inline')
  knitr::knit_hooks$set(inline = function(x) {
    res = hook_inline(x)
    if (is.numeric(x)) sprintf('$%s$', res) else res
  })
})

# define printf function
printf <- function(...)print(sprintf(...))
```

```{r libraries, include=FALSE}
# Import model libraries
library(pls)
library(randomForest)
library(gam)
library(glmnet)
library(ggplot2)
library(corrplot)
library(tidyverse)
library(caret)
library(mgcv)
library(Metrics)
library(visreg)
library(boot)
library('ggthemes') 
library('scales')
library('mice')
library('data.table')
library('gridExtra') 
library('GGally')
library('e1071')
```

# Introduction

## Description of the Problem

Being able to predict the price of a house tends to be an important skill for both the seller and consumers. For the seller, they could make better sales and consumers could have better understanding when they try to make a purchase. Therefore, in this project, we are planning to make prediction of house price based on the 79 different predictors provided by Kaggle dataset to determine values of residential homes in Ames, Iowa. We have noticed Sale Price has a typical right-skewed distribution, and decided to process it using two ways, logrithmic with base $e$ and square root for a distribution that is much closer to the shape of a Gaussian distribution for better performance in models like linear regression. In this analysis, we will perform random forest on original y-value for importance of variables and all the rest of models on both absolute and processed y-values to see which way would each model do better and provide an ensemble of models at the end of our study. 

## Description of the Dataset

In terms of the dataset, the entire data set consists of two pieces of data organized as training data set and test data set respectively. Whereas for each of the dataset, approximately 80 columns corresponding parameters would be evaluated with the prediction of house price. Some noteworthy predictors include the location classification, utilities, environment of neighborhood, house style and condition, area, year of built, and number of functioning rooms. There are over 1400 row of data points in both the training data set and the test data set. The sale prices in the train dataset are given as a parameter in the form of five or six figure full flat integers. The test data set will be applied to different regression models in order to distinguish the disparities of different model performances. 
    
## Approaches

Given that our data is aimed at predicting Sale Price of a house, it is unreasonable to require a model to fit the exact value of the dataset but only to reach an estimation within a certain range. Therefore, we have decided to use both regression and classification approaches to look at the problem on both the original and processed value. For regression method, we are going to look at if a prediction is within the range of the actual price $\pm 5\%$, we will say it is an accurate prediction. For classification prediction, we will be tagging the data into several different groups, and would be fitting the threshold accordingly with models like SVM and K-Means clustering. 

# Data Processing

## Read in Data

We have chosen to eliminate the Id column from this dataset because Id has nothing to do with our prediction and would mess up our prediction. We save data in "train.csv"" from Kaggle into a variable named \textbf{HousePricing} for further processing and we will separate it into training and testing set. For each model Bootstrapping will be performed before each model's training process.   

```{r readData}
HousePricing = read.csv("./SourceData/train_new.csv")
HousePricing = subset(HousePricing,select=-Id)
```

## Data Exploration

```{r dimensionOfData}
# The number of columns and rows
paste("Original training data set has",dim(HousePricing)[1], "rows and", dim(HousePricing)[2], "columns")

# The percentage of data missing in train
paste("The percentage of data missing in the original training data set is ", round(sum(is.na(HousePricing)) / (nrow(HousePricing) *ncol(HousePricing)),4)*100,"%",sep = "")

# The number of duplicated rows
paste("The number of duplicated rows are", nrow(HousePricing) - nrow(unique(HousePricing)))
```

```{r numOfNumericAndFactors}
a = sum(sapply(HousePricing[,1:80],typeof) == "character")
printf("Number of Factors: %i", a)

printf("Number of Numeric: %d", sum(sapply(HousePricing[,1:80],typeof) == "integer"))

```

### Target Varaible vs. Predictors

```{r targetvspredictors}
summary(HousePricing$SalePrice)
summary(HousePricing$GrLivArea)
hist(HousePricing$SalePrice,col="blue",breaks = 25,main = "Distribution of SalePrice", xlab = "Sale Price")
```

\textbf{Conclusion}

It deviates from normal distribution and it is right skewed

### Plotting 'GrLivArea' too see if there are any outliers

```{r outlier, fig.width=5, fig.height=3}
qplot(GrLivArea, SalePrice, data= HousePricing,col=GrLivArea>4000,xlab = "GrLivArea", ylab="Sale Price",main = "Living Area vs. Sale Price")


hist(HousePricing$GrLivArea,breaks = 20,xlab="Living area",col = "dark red",main = "Frequency of Living area square feet")
```

### Average Price of Each Neighborhood

```{r avgPrice}
nSalesPrice = HousePricing[,c("Neighborhood","SalePrice")]
avgPriceN = aggregate(nSalesPrice[, 2], list(nSalesPrice$Neighborhood), mean)
ggplot(avgPriceN, aes(x=Group.1, y=x)) + 
  geom_bar(stat = "identity") +
  coord_flip() + scale_fill_brewer(palette = "Set1") +
  theme(legend.position="none")

```

## Feature Engineering

In this section, we convert all missing value based on the following rules:

\begin{enumerate}
\item Categorical: fill in most common
\item Numeric: fill in median/average
\end{enumerate}

Convert all train to HousePricing

```{r simplePlot}
ggplot(HousePricing,aes(x=GrLivArea,y=SalePrice))+geom_point()
```
```{r featuerEngineering, include=FALSE}
# remove ID column
HousePricing$Id = NULL
HousePricing = HousePricing[HousePricing$GrLivArea<4500,]

# for using later
numericVars <- which(sapply(HousePricing, is.numeric))
numericVarNames <- names(numericVars) 

# find the columns contains NA and the number of NA values in each columns
NAcol <- which(colSums(is.na(HousePricing)) > 0)
sort(colSums(sapply(HousePricing[NAcol], is.na)), decreasing = TRUE)

# LotFrontage
# compute the median of neighbor, na.rm means compute medians without NA
neighbor_Median  = HousePricing %>%
  select(LotFrontage, Neighborhood) %>%
  group_by(Neighborhood) %>%
  summarise(LotFrontage = median(LotFrontage, na.rm = T))

# replace the LotFrontage NA with its neighbor's Lotfrontage's median.
for (i in 1:nrow(HousePricing))
{
  if(is.na(HousePricing$LotFrontage[i])){
    temp = HousePricing$Neighborhood[i]
    HousePricing$LotFrontage[i] = neighbor_Median$LotFrontage[neighbor_Median$Neighborhood == temp]
  }
}

# Alley, NA means no alley.
HousePricing$Alley[is.na(HousePricing$Alley)] = 'None'
HousePricing$Alley = as.factor(HousePricing$Alley)

HousePricing$Utilities = NULL
table(HousePricing$PoolQC)
HousePricing$PoolQC[is.na(HousePricing$PoolQC)] = "None"
HousePricing$PoolQC=recode(HousePricing$PoolQC,'None' = 0,'Fa' = 1,'TA' = 2,'Gd' = 3,'Ex' = 4)
# Fence
HousePricing$Fence[is.na(HousePricing$Fence)] = "None"
HousePricing$Fence = as.factor(HousePricing$Fence)
HousePricing$MiscFeature[is.na(HousePricing$MiscFeature)] = "None"
HousePricing$MiscFeature = as.factor(HousePricing$MiscFeature)

# garage
HousePricing$GarageYrBlt[is.na(HousePricing$GarageYrBlt)] <- HousePricing$YearBuilt[is.na(HousePricing$GarageYrBlt)]
HousePricing$GarageType[is.na(HousePricing$GarageType)] = "None"
HousePricing$GarageType = as.factor(HousePricing$GarageType)
HousePricing$GarageFinish[is.na(HousePricing$GarageFinish)] = "None"
HousePricing$GarageFinish=recode(HousePricing$GarageFinish,'None' = 0,'Unf' = 1,'RFn' = 2,'Fin' = 3)
HousePricing$GarageQual[is.na(HousePricing$GarageQual)] = "None"
HousePricing$GarageQual=recode(HousePricing$GarageQual,'None' = 0,'Po' = 1,'Fa' = 2,'TA' = 3,'Gd' = 4,'Ex' = 5)
HousePricing$GarageCond[is.na(HousePricing$GarageCond)] = "None"
HousePricing$GarageCond=recode(HousePricing$GarageCond,'None' = 0,'Po' = 1,'Fa' = 2,'TA' = 3,'Gd' = 4,'Ex' = 5)
HousePricing$FireplaceQu[is.na(HousePricing$FireplaceQu)] = "None"
HousePricing$FireplaceQu=recode(HousePricing$FireplaceQu,'None' = 0,'Po' = 1,'Fa' = 2,'TA' = 3,'Gd' = 4,'Ex' = 5)

#electric
HousePricing$Electrical[is.na(HousePricing$Electrical)] = "SBrkr"
HousePricing$Electrical = as.factor(HousePricing$Electrical)

# basement 
length(which(is.na(HousePricing$BsmtQual) & is.na(HousePricing$BsmtCond) & is.na(HousePricing$BsmtExposure) & is.na(HousePricing$BsmtFinType1) & is.na(HousePricing$BsmtFinType2)))
HousePricing[!is.na(HousePricing$BsmtCond) & (is.na(HousePricing$BsmtFinType1)|is.na(HousePricing$BsmtQual)|is.na(HousePricing$BsmtExposure)|is.na(HousePricing$BsmtFinType2)), c('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2')]
HousePricing$BsmtFinType2[333] = names(sort(table(HousePricing$BsmtFinType2),decreasing = TRUE))[1]
HousePricing$BsmtExposure[949] = names(sort(table(HousePricing$BsmtExposure),decreasing = TRUE))[1]
HousePricing$BsmtExposure[is.na(HousePricing$BsmtExposure)] = 'None'
HousePricing$BsmtExposure=recode(HousePricing$BsmtExposure,'None' = 0,'No' = 1,'Mn' = 2,'Av' = 3,'Gd' = 4)
HousePricing$BsmtQual[is.na(HousePricing$BsmtQual)] = 'None'
HousePricing$BsmtQual=recode(HousePricing$BsmtQual,'None' = 0,'Po' = 1,'Fa' = 2,'TA' = 3,'Gd' = 4,'Ex' = 5)
HousePricing$BsmtCond[is.na(HousePricing$BsmtCond)] = 'None'
HousePricing$BsmtCond=recode(HousePricing$BsmtCond,'None' = 0,'Po' = 1,'Fa' = 2,'TA' = 3,'Gd' = 4,'Ex' = 5)
HousePricing$BsmtFinType1[is.na(HousePricing$BsmtFinType1)] = 'None'
HousePricing$BsmtFinType1=recode(HousePricing$BsmtFinType1,'None' = 0,'Unf' = 1,'LwQ' = 2,'Rec' = 3,'BLQ' = 4,'ALQ' = 5, 'GLQ' = 6)
HousePricing$BsmtFinType2[is.na(HousePricing$BsmtFinType2)] = 'None'
HousePricing$BsmtFinType2=recode(HousePricing$BsmtFinType2,'None' = 0,'Unf' = 1,'LwQ' = 2,'Rec' = 3,'BLQ' = 4,'ALQ' = 5, 'GLQ' = 6)

# Mas
HousePricing$MasVnrType[is.na(HousePricing$MasVnrType)] = 'None'
HousePricing$MasVnrType = as.factor(HousePricing$MasVnrType)
HousePricing$MasVnrArea[(is.na(HousePricing$MasVnrArea))] = 0

HousePricing$MSZoning = as.factor(HousePricing$MSZoning)
HousePricing$Street = as.factor(HousePricing$Street)
HousePricing$LotShape=recode(HousePricing$LotShape,'IR3' = 0,'IR2' = 1,'IR1' = 2,'Reg' =2)
HousePricing$LotConfig = as.factor(HousePricing$LotConfig)

HousePricing$Condition1 = as.factor(HousePricing$Condition1)
HousePricing$Condition2 = as.factor(HousePricing$Condition2)
HousePricing$LandContour = as.factor(HousePricing$LandContour)
HousePricing$RoofStyle = as.factor(HousePricing$RoofStyle)
HousePricing$LandSlope=recode(HousePricing$LandSlope,'Sev' = 0,'Mod' = 1,'Gtl' = 2)

HousePricing$BldgType = as.factor(HousePricing$BldgType)
HousePricing$HouseStyle=as.factor(HousePricing$HouseStyle)


HousePricing$RoofMatl=as.factor(HousePricing$RoofMatl)
HousePricing$Exterior1st=as.factor(HousePricing$Exterior1st)
HousePricing$Exterior2nd=as.factor(HousePricing$Exterior2nd)
HousePricing$ExterQual=recode(HousePricing$ExterQual,'Po' = 0,'Fa' = 1,'TA' = 2,'Gd' = 3,'Ex' = 4)
HousePricing$ExterCond=recode(HousePricing$ExterCond,'Po' = 0,'Fa' = 1,'TA' = 2,'Gd' = 3,'Ex' = 4)

HousePricing$Foundation = as.factor(HousePricing$Foundation)
HousePricing$PavedDrive=recode(HousePricing$PavedDrive,'N' = 0,'P' = 1,'Y' = 2)
HousePricing$Heating = as.factor(HousePricing$Heating)
HousePricing$HeatingQC=recode(HousePricing$HeatingQC,'Po' = 0,'Fa' = 1,'TA' = 2,'Gd' = 3,'Ex' = 4)
HousePricing$CentralAir=recode(HousePricing$CentralAir,'N' = 0,'Y' = 1)


HousePricing$KitchenQual=recode(HousePricing$KitchenQua,'Po' = 0,'Fa' = 1,'TA' = 2,'Gd' = 3,'Ex' = 4)
HousePricing$Functional=recode(HousePricing$Functional,'Sal' = 0,'Sev' = 1,'Maj2' = 2,'Maj1' = 3,'Mod' = 4,'Min2' = 5,'Min1' = 6,'Typ' = 7)
HousePricing$Neighborhood = as.factor(HousePricing$Neighborhood)
HousePricing$SaleType = as.factor(HousePricing$SaleType)
HousePricing$SaleCondition = as.factor(HousePricing$SaleCondition)
HousePricing$MoSold = NULL
HousePricing$MSSubClass = as.factor(HousePricing$MSSubClass)
HousePricing$MSSubClass=recode(HousePricing$MSSubClass,'20' = '1-STORY 1946+',
                               '30' = '1-STORY 1945-','40' = '1-STORY Unf Attic',
                               '45' = "1/2 STORY Unf Attic",'50' = '1/2 STORY Fin',
                               '60' = '2-STORY+','70' = '2-STORY 1945-','80' = 'SPLIT OR MULTI-LEVEL',
                               '85' = 'SPLIT FOYER','90' = 'DUPLEX', '120' = '1-STORY PUD 1946+',
                               '150' = '1/2 STORY PUD','160' = '2-STORY PUD 1946+',
                               '180' = 'PUD - MULTILEVEL',' 190' = '2 FAMILY CONVERSION')




```

Correlation between the numerical variables

```{r corNum}
# # draw a plot of correlation between numerical variables in original data
# #which(sapply(HousePricing, is.numeric))
# numericVars = which(sapply(HousePricing, is.numeric))#numericVars
# factorVars = which(sapply(HousePricing, is.factor))
# numVar = HousePricing[,numericVars]
# cor_numVar =(cor(numVar))
# cor_sorted = as.matrix(sort(cor_numVar[,"SalePrice"],decreasing = TRUE))
# #cor_sorted
# CorHigh <- names(which(apply(cor_sorted, 1, function(x) abs(x)>0.5)))
# cor_numVar <- cor_numVar[CorHigh, CorHigh]
# corrplot.mixed(cor_numVar, tl.col="black", tl.pos = "lt", tl.cex = 0.7,cl.cex = .7, number.cex=.7)
# 
# # draw a importance plot of all predictors in the original data
# set.seed(2018)
# quick_RF <- randomForest(x=HousePricing[,-78], y=HousePricing$SalePrice, ntree=100,importance=TRUE)
# imp_RF <- importance(quick_RF)
# imp_DF <- data.frame(Variables = row.names(imp_RF), MSE = imp_RF[,1])
# imp_DF <- imp_DF[order(imp_DF$MSE, decreasing = TRUE),]
# ggplot(imp_DF[1:15,], aes(x=reorder(Variables, MSE), y=MSE, fill=MSE)) + geom_bar(stat = 'identity') + labs(x = 'Variables', y= '% increase MSE if variable is randomly permuted') + coord_flip() + theme(legend.position="none")

```
```{r ffe}
########################### further feature engineer ###################
# whether remod 
HousePricing$Remod = ifelse(HousePricing$YearBuilt == HousePricing$YearRemodAdd,0,1)
# the age of house 
HousePricing$Age = as.numeric(HousePricing$YrSold) - HousePricing$YearRemodAdd 
# whether is new
HousePricing$isnew = ifelse(HousePricing$YrSold == HousePricing$YearBuilt,1,0)
# total area.
HousePricing$TotalSqFeet = HousePricing$GrLivArea+HousePricing$TotalBsmtSF
# count the totol number of bathroom in the hourse
HousePricing$TotBathrooms <- HousePricing$FullBath + (HousePricing$HalfBath*0.5) + HousePricing$BsmtFullBath + (HousePricing$BsmtHalfBath*0.5)

HousePricing$TotalPorchSF <- HousePricing$OpenPorchSF + HousePricing$EnclosedPorch + HousePricing$X3SsnPorch + HousePricing$ScreenPorch

# draw a correlation plot after combine some variable
numericVars = which(sapply(HousePricing, is.numeric))#numericVars
factorVars = which(sapply(HousePricing, is.factor))
numVar = HousePricing[,numericVars]
cor_numVar =(cor(numVar))
cor_sorted = as.matrix(sort(cor_numVar[,"SalePrice"],decreasing = TRUE))
#cor_sorted
CorHigh <- names(which(apply(cor_sorted, 1, function(x) abs(x)>0.5)))
cor_numVar <- cor_numVar[CorHigh, CorHigh]
corrplot.mixed(cor_numVar, tl.col="black", tl.pos = "lt", tl.cex = 0.7,cl.cex = .7, number.cex=.7)

# choose some true numerical variable to normalize(not include the encoded part)
numericVarNames <- numericVarNames[!(numericVarNames %in% c('MSSubClass', 'MoSold', 'YrSold', 'SalePrice', 'OverallQual', 'OverallCond'))]
numericVarNames <- append(numericVarNames, c('Age', 'TotalPorchSF', 'TotBathrooms', 'TotalSqFeet'))

# delete some highly correlated variables
HousePricing = subset(HousePricing, select = -c(GrLivArea,ExterQual,GarageArea,X1stFlrSF,
                                           TotRmsAbvGrd,TotalBsmtSF,GarageYrBlt,FullBath,
                                           HalfBath,YearRemodAdd,BsmtHalfBath,BsmtFullBath))
DFnumeric <- HousePricing[, names(HousePricing) %in% numericVarNames]
DFfactors <- HousePricing[, !(names(HousePricing) %in% numericVarNames)]
DFfactors <- DFfactors[, names(DFfactors) != 'SalePrice']

########################## Normalizing the numerical data #########################
predf = scale(DFnumeric,center = T,scale = T)

```

As discussed before, we have decided to use logrithmic with base $e$ and square root to process the data. We have also saved $15\%$ of our data into a variable named vault for the final test of each model. 

```{r getFinalData}
############ one-hot encoding and combine with scaled numerical data #######################################
dfdummies = model.matrix(~.-1,DFfactors) %>% as.data.frame()
newdata = cbind(predf,dfdummies)
newdata$SalePrice = HousePricing$SalePrice

set.seed(1)
vault = sample(1:nrow(newdata), nrow(newdata)*0.15)
dVault = newdata[vault,]
newdata = newdata[-vault,]
oriHouseP = newdata
oriHouseP$SalePrice = newdata$SalePrice
sumOri = summary(oriHouseP$SalePrice)
sumOri
logHouseP = newdata
logHouseP$SalePrice = log(newdata$SalePrice)
sumLog = summary(logHouseP$SalePrice)
hist(logHouseP$SalePrice)
sumLog
sqrtHouseP = newdata
sqrtHouseP$SalePrice = '^'(newdata$SalePrice,1/4)
sumSqrt = summary(sqrtHouseP$SalePrice)
hist(sqrtHouseP$SalePrice)
sumSqrt
```

```{r toFactor}
toFac <- function(original){
  result = original
  result$SalePrice[which(result$SalePrice<=summary(original$SalePrice)["1st Qu."])] = 0
  result$SalePrice[which(result$SalePrice>=summary(original$SalePrice)["3rd Qu."])] = 2
  result$SalePrice[which(result$SalePrice<summary(original$SalePrice)["3rd Qu."] & result$SalePrice>summary(original$SalePrice)["1st Qu."])] = 1
  result$SalePrice = as.factor(result$SalePrice)
  print(summary(result$SalePrice))
  return(result)
}

oriHousePCl = toFac(oriHouseP)
logHousePCl = toFac(logHouseP)
sqrtHousePCl = toFac(sqrtHouseP)
```



```{r prepareNewData, fig.width=5,fig.height=3}
# numvar = which(sapply(combined_data, is.numeric))
# catvar = which(sapply(combined_data, is.factor))
# numdata = combined_data[,numvar]
# numcor =(cor(numdata))
# corsorted = as.matrix(sort(numcor[,"SalePrice"],decreasing = TRUE))
# CorHigh <- names(which(apply(corsorted, 1, function(x) abs(x)>0.5)))
# numcor <- numcor[CorHigh, CorHigh]
# corrplot.mixed(numcor, tl.col="black", tl.pos = "lt", tl.cex = 0.7,cl.cex = .7, number.cex=.7)
# temp = subset(combined_data,select=-SalePrice)
# # standardize numerical data 
# numeric = select_if(temp,is.numeric)
# stnumer = scale(numeric,center = T,scale = T)
# convFact = select_if(temp,is.factor)
# # one hot
# convFact = model.matrix(~.-1,convFact) %>% data.frame()
# SalePrice = log(combined_data$SalePrice)
# # put standardized numerical data and categorical data in one data?
# newdata = cbind(stnumer,convFact,SalePrice)
```

```{r bootstrapingFunc, include=FALSE}
bsF<- function(datadf, randomizer){
  set.seed(randomizer)
  sample = sample(dim(datadf)[1],dim(datadf)[1],replace = T)
  btnewdata = datadf[sample,]
  return(btnewdata)
}
newOriHouseP = bsF(oriHouseP, 1234)
newLogHouseP = bsF(logHouseP, 1234)
newSqrtHouseP = bsF(sqrtHouseP, 1234)
newOriHousePCl = bsF(oriHousePCl, 1234)
newLogHousePCl = bsF(logHousePCl, 1234)
newSqrtHousePCl = bsF(sqrtHousePCl, 1234)
```

## PCA

In order to reduce the dimension convincing by finding a proper Manifold, we apply PCA method. Fortunately, we successfully reduced the dimension from 216 to no more than 32. 32 PCs have totally 99.997% of variance proportion. 29 PCs are enough for totally getting 85% of variance proportion.

```{r PCA}
# x_train_ori.pr = prcomp(x_train_ori)
# print(summary(x_train_ori.pr))
```

In the summary we can find all variance proportions behind 32th PC are 0, which means 32 PCs are totally enough important to describe all features of the original data. In fact, we can take less than 32 PCs and
find the best number of PCs with cross validation in training process.
The sum of (0.1627, 0.1073, 0.105, 0.07973, 0.05255, 0.05155, 0.03586, 0.03352, 0.03141, 0.02978, 0.02679, 0.02574, 0.02376, 0.02327, 0.02185, 0.02102, 0.01885, 0.01826, 0.01709, 0.01591, 0.01533, 0.01396, 0.01287, 0.0110, 0.01046, 0.00921, 0.00687, 0.00639, 0.00596, 0.00374, 0.00187, 0.00037) is  0.99997, which means 32 PCs totally have 99.997% of variance proportion. Moreover, 29 PCs are enough for totally getting 85% of variance proportion.
```{r pcaplot1}
# plot(x_train_ori.pr,type = "l", main = NULL)
```

In this plot, you can see an elbow at 7. Perhaps it can be a good number of PCs.

```{r pcaplot2}
y = list(0.1627, 0.1073, 0.105, 0.07973, 0.05255, 0.05155, 0.03586, 0.03352, 0.03141, 0.02978, 0.02679, 0.02574, 0.02376, 0.02327, 0.02185, 0.02102, 0.01885, 0.01826, 0.01709, 0.01591, 0.01533, 0.01396, 0.01287, 0.0110, 0.01046, 0.00921, 0.00687, 0.00639, 0.00596, 0.00374, 0.00187, 0.00037)
x = c(1:32)
plot(x,y, type="l")
```

This is generated by plotting all variance proportions of 32 PCs. We can see the elbow at 7 more clearly than the previous one. However, the square of tail on the right side of 7 is quite thick, which means maybe a number on the tail can be the best one for training. Last but not least, the exact best number can only be revealed by cross validation.

```{r newdata}
# New data list cleansed by PCA size of 868*32, which can take the place of original data. 
# Each column is one PC, PC's importance decreases by column number.
# Try CV on number of PC you drop from right to left and know how many PCs are best to use.
# newdata = x_train_ori.pr$x[,1:32]
# print(nrow(x_train_ori))
# print(ncol(x_train_ori))
# print(nrow(newdata))
# print(ncol(newdata))
# print(newdata[,1]) # 1st PCA, the most important PC
# print(newdata[,32]) # 32th PCA, the least important PC
```

This is to show the effect of dimension reduction and how can we take and use the new data modified by PCA method.


## Seperate into Test and Training Set

Spearate by 70% train, 30% test. 

```{r saveToCSV}
toCsv <- function(df, fileName){
  set.seed(10)
  randS = sample(1:nrow(df), nrow(df)*0.7)
  train = df[randS,]
  test = df[-randS,]
  write.csv(train,paste("./SourceData/train_",fileName, ".csv",sep=""), row.names = FALSE)
  write.csv(test,paste("./SourceData/test_",fileName, ".csv",sep=""), row.names = FALSE)
}

toCsv(oriHouseP, "original")
toCsv(logHouseP, "log")
toCsv(sqrtHouseP, "sqrt")
```



```{r oriTestTrainReg, include=FALSE}
test_ori = read.csv("./SourceData/test_original.csv")
y_test_ori = test_ori$SalePrice
x_test_ori = subset (test_ori, select = -SalePrice)
train_ori = read.csv("./SourceData/train_original.csv")
y_train_ori = train_ori$SalePrice
x_train_ori = subset (train_ori, select = -SalePrice)
y_train_ori = as.numeric(y_train_ori)
y_test_ori = as.numeric(y_test_ori)
summary(y_train_ori)
```

```{r logTestTrainReg, include=FALSE}
test_log = read.csv("./SourceData/test_log.csv")
y_test_log = test_ori$SalePrice
x_test_log = subset (test_log, select = -SalePrice)
train_log = read.csv("./SourceData/train_log.csv")
y_train_log = train_log$SalePrice
x_train_log = subset (train_log, select = -SalePrice)
y_train_log = as.numeric(y_train_log)
y_test_log = as.numeric(y_test_log)
summary(y_train_log)
```

```{r sqrtTestTrainReg, include=FALSE}
test_sqrt = read.csv("./SourceData/test_sqrt.csv")
y_test_sqrt = test_sqrt$SalePrice
x_test_sqrt = subset (test_sqrt, select = -SalePrice)
train_sqrt = read.csv("./SourceData/train_sqrt.csv")
y_train_sqrt = train_sqrt$SalePrice
x_train_sqrt = subset (train_sqrt, select = -SalePrice)
y_train_sqrt = as.numeric(y_train_sqrt)
y_test_sqrt = as.numeric(y_test_sqrt)
summary(y_train_sqrt)
```


# Prediction Algorithms

We choose to use PCR, Random Forest, GAM, Lasso and Ridge, Splines and Linear Regression to look at how each model would be suitable for our regression analysis. 

Each model needs a cross validation algorithm
Remember to report RMSE

## Regression Methods

### 1. Linear Regression 

#### Explanation

We have chosen this model to understand how each numeric variable is linear related to our House Price prediction.
```{r lr ori}
# ori
k = 5
lm_ori_accuracy = rep(0,k)
for (i in 1:k){
  set.seed(100+i)
  sample = sample(nrow(train_ori),nrow(train_ori),replace = T)
  ori_train = train_ori[sample,]
  train = sample(nrow(ori_train),0.7*nrow(ori_train))
  training_dataset = ori_train[train,]
  validation_dataset = ori_train[-train,]
  lm_ori_model = glm(formula = SalePrice ~.,data = training_dataset)
  lm_ori_pred = predict(lm_ori_model,newdata = validation_dataset)
  lm_ori_accuracy[i] = mean(abs((validation_dataset$SalePrice)- (lm_ori_pred))/(validation_dataset$SalePrice)<=0.05)
}
lm_ori_aver_accuracy = mean(lm_ori_accuracy)
```

```{r lr log}
k = 5
lm_log_accuracy = rep(0,k)
for (i in 1:k){
  set.seed(111+i)
  sample = sample(nrow(train_log),nrow(train_log),replace = T)
  log_train = train_log[sample,]
  train = sample(nrow(log_train),0.7*nrow(log_train))
  training_dataset = log_train[train,]
  validation_dataset = log_train[-train,]
  lm_log_model = glm(formula = SalePrice ~.,data = training_dataset)
  lm_log_pred = predict(lm_log_model,newdata = validation_dataset)
  lm_log_pred = exp(lm_log_pred)
  lm_log_accuracy[i] = mean(abs(exp(validation_dataset$SalePrice)-(lm_log_pred))/exp(validation_dataset$SalePrice)<=0.05)
}
lm_ori_aver_accuracy = mean(lm_ori_accuracy)
```

```{r lr sqrt}
lm_sqrt_accuracy = rep(0,k)
for (i in 1:k){
  set.seed(150+i)
  sample = sample(nrow(train_sqrt),nrow(train_sqrt),replace = T)
  sqrt_train = train_sqrt[sample,]
  train = sample(nrow(sqrt_train),0.7*nrow(sqrt_train))
  training_dataset = sqrt_train[train,]
  validation_dataset = sqrt_train[-train,]
  lm_sqrt_model = glm(formula = SalePrice ~.,data = training_dataset)
  lm_sqrt_pred = predict(lm_sqrt_model,newdata = validation_dataset)
  lm_sqrt_pred = lm_sqrt_pred^4
  lm_sqrt_accuracy[i] = mean(abs((validation_dataset$SalePrice)^4- (lm_sqrt_pred))/(validation_dataset$SalePrice)^4<=0.05)
}
lm_sqrt_aver_accuracy = mean(lm_sqrt_accuracy)
```

#### Check Accuracy

```{r lr2}
# Multiple R-squared: 0.9475, 
# Adjusted R-squared: 0.9345  
# F-statistic:  73.09  on 289 and 1170 DF, 
# p-value: < 2.2e-16

#accuracy for original 
lm_ori_accuracy
lm_ori_aver_accuracy
printf("We have the accuracy of the linear model approximately %.2f%%", lm_ori_aver_accuracy*100)

# accuracy for log
lm_log_accuracy
lm_log_aver_accuary = mean(lm_log_accuracy)
printf("We have the accuracy of the linear model after log transformation approximately %.2f%%", lm_log_aver_accuary*100)

## accuracy for sqrt
lm_sqrt_accuracy
lm_sqrt_aver_accuracy = mean(lm_sqrt_accuracy)
printf("We have the accuracy of the linear model after sqrt transformation approximately %.2f%%", lm_sqrt_aver_accuracy*100)

lm_accuracy_df = data.frame(lm = c('accuracy'),ori_accuracy = c(lm_ori_aver_accuracy),
                               log_accuracy = c(lm_log_aver_accuary),
                               sqrt_accuracy = c(lm_sqrt_aver_accuracy))
#lm_pred  = exp(lm_pred)
#result_lm_model = data.frame(Id = testing_data$Id, SalePrice = lm_pred)
```

### 2. Random Forest

#### Explanation

We have chosen this model because random forest is based on a collection of decision trees that could help us get better understanding of which tree and division contribute to which section such that we could have a better picture of the overall importance of each different factor in the prediction.

#### Prepare Model

We have $199$ independent variables in the data set, therefore we have set mtry(Number of randomly selected variables for each split) to be the square root of that number for maximum performance of the model. 

The following is the result from Random Forest algorithm: 

```{r rf-preparemodel}
# Need to figure out how many independent variables then set mtry
# set mtry to be square root of total number of independent variables
totalIV = length(colnames(train_ori))
rfTrain=randomForest(SalePrice~.,data=train_ori, mtry=sqrt(totalIV),importance =TRUE)
pander(rfTrain)
```

#### Check Accuracy

We then need to check accuracy, as assumed before, we would look at whether the predicted data is within the given range. The following is the result. 

```{r rfAccuracy}
rfYhat_train = predict(rfTrain, newdata=x_train_ori)
accuracy = mean(((y_train_ori - rfYhat_train)/y_train_ori<=0.2) & ((y_train_ori - rfYhat_train)/y_train_ori>=-0.05))
printf("We have the accuracy of the model approximately %.2f%%", accuracy*100)
rfYhat = predict(rfTrain, newdata=x_test_ori)
#table(y_test, rfYhat)
accuracy = mean(((y_test_ori - rfYhat)/y_test_ori<=0.2) & ((y_test_ori - rfYhat)/y_test_ori>=-0.05))
printf("We have the accuracy of the model approximately %.2f%%", accuracy*100)
```

#### Error Metrics

Then let us take a look at the MSE of this model: 
```{r rfMSE}
printf("We have the MSE of the model approximately equal to %.5f", mean((rfYhat-test_ori$SalePrice)^2))

```

#### Variable Importance

Here we are going to show the top 10 most important variables in predicting sale price of a house.

```{r rfVarImportance1, fig.width=5,fig.height=3}
importance = importance(rfTrain)
varImportance = head(data.frame(Variables = row.names(importance),
 Importance =round(importance[, "%IncMSE"],1)),10)
rankImportance=varImportance%>%mutate(Rank=paste('#',dense_rank(desc(Importance))))
ggplot(rankImportance,aes(x=reorder(Variables,Importance),
 y=Importance,fill=Importance))+ 
 geom_bar(stat='identity') + 
 geom_text(aes(x = Variables, y = 0.5, label = Rank),
 hjust=0, vjust=0.55, size = 4, colour = 'white') +
 labs(x = 'Variables') +
 coord_flip() + 
 theme_classic()

## another plot
importance = importance(rfTrain)
importancedf =  data.frame(Variables = row.names(importance), MSE = importance[,1])
importancedf <- importancedf[order(importancedf$MSE, decreasing = TRUE),]
ggplot(importancedf[1:10,], aes(x=reorder(Variables, MSE), y=MSE, fill=MSE)) + geom_bar(stat = 'identity') + labs(x = 'Variables', y= '% increase MSE if variable is randomly permuted') + coord_flip() + theme(legend.position="none")




```


From the random forest analysis, we have discovered that the top three most important factors for predicting sale price are the following:

\begin{enumerate}
\item OverallQual (Overall Quality of the building)
\item ExterQual (Evaluates the quality of the material on the exterior)
\item YearBuilt (The year the house is built)
\end{enumerate}

#### Cross Validation

In the cross validation, we have chosen to look at $R^2$, RMSE and MAE. 

```{r RFCV}
set.seed(123) 
  
# computing model performance metrics 
pander(data.frame( R2 = R2(rfYhat, y_test_ori), 
            RMSE = RMSE(rfYhat, y_test_ori), 
            MAE = MAE(rfYhat, y_test_ori)), title="Cross Validation for Random Forest")
```

### 3. PCR (Iris)

```{r PCR}
set.seed(2)
# Need help with PCA analysis
# pcr.fit=pcr(SalePrice~., data=HousePricing, scale=TRUE, validation ="CV")
```

#### Cross Validation

### 4. Ridge Regression

#### Explanation

The reason we choose Ridge regression model is Ridge regression is very similar to linear regression, both try to minimize the RSS, but ridge regression has a penalty term, this could help us to prevent overfitting when add more predictors.

#### Prepare Model

1. Bootstrap Training Data

```{r ridgebs}
# set.seed(2)
# bs = sample(dim(train)[1],dim(train)[1],replace = T)
# train = train[bs,]
# 
# X_train = model.matrix(SalePrice~.,data = train)[,-1]
# X_test = model.matrix(SalePrice~.,test)[,-1]
# y_train = train$SalePrice
# y_test = test$SalePrice
```

1. First, we set initial alpha to 1 to fit the ridge regression,and set the values of initial lambda ranging from 10^10 to 10^(-2), essentially covering the full range of scenarios from the null model containing only the intercept, to the least squares fit.

```{r ridge }
# original 
Ridge_ori_accuracy= rep(0,k)
Ridge_ori_lambda = rep(0,k)
Ridge_ori_R2 = rep(0,k)
Ridge_ori_MAE = rep(0,k)
Ridge_ori_RMSE = rep(0,k)
Ridge_ori_coef = 0
for ( i in 1:k){
  set.seed(220+i)
  sample = sample(nrow(train_ori),nrow(train_ori),replace = T)
  ori_train = train_ori[sample,]
  train = sample(nrow(ori_train),0.7*nrow(ori_train))
  training_dataset = ori_train[train,]
  validation_dataset = ori_train[-train,]
  X_train = model.matrix(SalePrice~.,data = training_dataset)[,-1]
  X_test = model.matrix(SalePrice~.,validation_dataset)[,-1]
  y_train = training_dataset$SalePrice
  y_test = validation_dataset$SalePrice
  set.seed(1100+i)
  grid=10^seq(10,-2, length =100)
  Ridge.ori.Alpha=0
  Ridge.ori.Fit = glmnet(X_train, y_train, alpha=Ridge.ori.Alpha, lambda=grid)
  Ridge.ori.Fitcv = cv.glmnet(X_train, y_train, alpha = Ridge.ori.Alpha,nfolds = 10,type.measure = 'deviance')
  Ridge.ori.lambda = Ridge.ori.Fitcv$lambda.min
  Ridge_ori_lambda[i] = Ridge.ori.lambda
  Ridge.ori.Pred <- predict(Ridge.ori.Fit, s= Ridge.ori.lambda, newx = X_test)
  Ridge.ori.coef = predict(Ridge.ori.Fit, s = Ridge.ori.lambda, type="coefficients")
  Ridge_ori_accuracy[i] = mean(abs((y_test) - (Ridge.ori.Pred))/(y_test) <=0.05)
  Ridge_ori_R2[i] = R2(Ridge.ori.Pred,(y_test))
  Ridge_ori_RMSE[i]= RMSE(Ridge.ori.Pred,(y_test))
  Ridge_ori_MAE[i] = MAE(Ridge.ori.Pred,(y_test))
}
Ridge_ori_aver_lambda = mean(Ridge_ori_lambda)
Ridge_ori_aver_accuracy = mean(Ridge_ori_accuracy)
Ridge_ori_aver_RMSE = mean(Ridge_ori_RMSE)
Ridge_ori_aver_R2 = mean(Ridge_ori_R2)
Ridge_ori_aver_MAE = mean(Ridge_ori_MAE)
Ridge_ori_aver_MAE

# log transformation
Ridge_log_accuracy= rep(0,k)
Ridge_log_lambda = rep(0,k)
Ridge_log_R2 = rep(0,k)
Ridge_log_MAE = rep(0,k)
Ridge_log_RMSE = rep(0,k)
Ridge_log_coef = 0
for ( i in 1:k){
  set.seed(200+i)
  sample = sample(nrow(train_log),nrow(train_log),replace = T)
  log_train = train_log[sample,]
  train = sample(nrow(log_train),0.7*nrow(log_train))
  training_dataset = log_train[train,]
  validation_dataset = log_train[-train,]
  X_train = model.matrix(SalePrice~.,data = training_dataset)[,-1]
  X_test = model.matrix(SalePrice~.,validation_dataset)[,-1]
  y_train = training_dataset$SalePrice
  y_test = validation_dataset$SalePrice
  set.seed(1234+i)
  grid=10^seq(10,-2, length =100)
  Ridge.log.Alpha=0
  Ridge.log.Fit = glmnet(X_train, y_train, alpha=Ridge.log.Alpha, lambda=grid)
  Ridge.log.Fitcv = cv.glmnet(X_train, y_train, alpha = Ridge.log.Alpha,nfolds = 10,type.measure = 'deviance')
  Ridge.log.lambda = Ridge.log.Fitcv$lambda.min
  Ridge_log_lambda[i] = Ridge.log.lambda
  Ridge.log.Pred <- predict(Ridge.log.Fit, s= Ridge.log.lambda, newx = X_test)
  Ridge.log.Pred = exp(Ridge.log.Pred)
  Ridge.log.coef = predict(Ridge.log.Fit, s = Ridge.log.lambda, type="coefficients")
  Ridge_log_accuracy[i] = mean(abs(exp(y_test) - (Ridge.log.Pred))/exp(y_test) <=0.05)
  Ridge_log_R2[i] = R2(Ridge.log.Pred,exp(y_test))
  Ridge_log_RMSE[i]= RMSE(Ridge.log.Pred,exp(y_test))
  Ridge_log_MAE[i] = MAE(Ridge.log.Pred,exp(y_test))
}
Ridge_log_aver_lambda = mean(Ridge_log_lambda)
Ridge_log_aver_accuracy = mean(Ridge_log_accuracy)
Ridge_log_aver_RMSE = mean(Ridge_log_RMSE)
Ridge_log_aver_R2 = mean(Ridge_log_R2)
Ridge_log_aver_MAE = mean(Ridge_log_MAE)

# sqrt 
Ridge_sqrt_accuracy= rep(0,k)
Ridge_sqrt_lambda = rep(0,k)
Ridge_sqrt_R2 = rep(0,k)
Ridge_sqrt_MAE = rep(0,k)
Ridge_sqrt_RMSE = rep(0,k)
Ridge_sqrt_coef = 0
for ( i in 1:k){
  set.seed(200+i)
  sample = sample(nrow(train_sqrt),nrow(train_sqrt),replace = T)
  sqrt_train = train_sqrt[sample,]
  train = sample(nrow(sqrt_train),0.7*nrow(sqrt_train))
  training_dataset = sqrt_train[train,]
  validation_dataset = sqrt_train[-train,]
  X_train = model.matrix(SalePrice~.,data = training_dataset)[,-1]
  X_test = model.matrix(SalePrice~.,validation_dataset)[,-1]
  y_train = training_dataset$SalePrice
  y_test = validation_dataset$SalePrice
  set.seed(1000+i)
  grid=10^seq(10,-2, length =100)
  Ridge.sqrt.Alpha=0
  Ridge.sqrt.Fit = glmnet(X_train, y_train, alpha=Ridge.sqrt.Alpha, lambda=grid)
  Ridge.sqrt.Fitcv = cv.glmnet(X_train, y_train, alpha = Ridge.sqrt.Alpha,nfolds = 10,type.measure = 'deviance')
  Ridge.sqrt.lambda = Ridge.sqrt.Fitcv$lambda.min
  Ridge_sqrt_lambda[i] = Ridge.sqrt.lambda
  Ridge.sqrt.Pred <- predict(Ridge.sqrt.Fit, s= Ridge.sqrt.lambda, newx = X_test)
  Ridge.sqrt.Pred = (Ridge.sqrt.Pred)^4
  Ridge.sqrt.coef = predict(Ridge.sqrt.Fit, s = Ridge.sqrt.lambda, type="coefficients")
  Ridge_sqrt_accuracy[i] = mean(abs((y_test)^4 - (Ridge.sqrt.Pred))/(y_test)^4 <=0.05)
  Ridge_sqrt_R2[i] = R2(Ridge.sqrt.Pred,exp(y_test))
  Ridge_sqrt_RMSE[i]= RMSE(Ridge.sqrt.Pred,exp(y_test))
  Ridge_sqrt_MAE[i] = MAE(Ridge.sqrt.Pred,exp(y_test))
}
Ridge_sqrt_aver_lambda = mean(Ridge_sqrt_lambda)
Ridge_sqrt_aver_accuracy = mean(Ridge_sqrt_accuracy)
Ridge_sqrt_aver_RMSE = mean(Ridge_sqrt_RMSE)
Ridge_sqrt_aver_R2 = mean(Ridge_sqrt_R2)
Ridge_sqrt_aver_MAE = mean(Ridge_sqrt_MAE)
```

1. Then we use cross validation to choose the optimal lambda for Ridge Regression, as the following:

```{r ridgeOplam}
# lambda of original 
Ridge_ori_lambda
Ridge_ori_aver_lambda
# lambda of log transformation 
Ridge_log_lambda
Ridge_log_aver_lambda
# lambda of sqrt transformation
Ridge_sqrt_lambda
Ridge_sqrt_aver_lambda

## with cross valdiation, we get the optimal lambda is 
Ridge_ori_aver_lambda
Ridge_log_aver_lambda
Ridge_sqrt_aver_lambda
par(mfrow=c(2,2))
# plot(Ridge_ori_Fitcv)
# plot(Ridge_log_Fitcv)
# plot(Ridge_sqrt_Fitcv)
par(mfrow = c(1,1))
```

#### Check Accuracy

We then need to check accuracy, as assumed before, we would look at whether the predicted data is within $\pm 5\%$. The following is the result.

```{r ridgeAcc}
# original accuracy
Ridge_ori_accuracy
Ridge_ori_aver_accuracy
printf("Accuracy of Ridge is approximately %.2f%%", Ridge_log_aver_accuracy*100)
# log accuracy
Ridge_log_accuracy
Ridge_log_aver_accuracy = mean(Ridge_log_accuracy)
printf("Accuracy of Ridge with Log Transformation is approximately %.2f%%", Ridge_log_aver_accuracy*100)
# sqrt accuracy
Ridge_sqrt_accuracy
Ridge_sqrt_aver_accuracy
printf("Accuracy of Ridge with Sqrt Transformation is approximately %.2f%%", Ridge_sqrt_aver_accuracy*100)

# accuracy dataframe
Ridge_accuracy_df = data.frame(Ridge = c('accuracy'),ori_accuracy=c(Ridge_ori_aver_accuracy),
                               log_accuracy = c(Ridge_log_aver_accuracy),
                               sqrt_accuracy = c(Ridge_sqrt_aver_accuracy))
```

#### Cross Validation

Then let us take a look at the MSE of this model:

```{r ridgeCV}
# ori
Ridge_ori_RMSE
Ridge_ori_aver_RMSE = mean(Ridge_ori_RMSE)
Ridge_ori_aver_R2 = mean(Ridge_ori_R2)
Ridge_ori_aver_MAE = mean(Ridge_ori_MAE)
pander(data.frame(R2 = Ridge_ori_aver_R2,RMSE = Ridge_ori_aver_RMSE,MAE = Ridge_ori_aver_MAE),title="Cross Validation of Ridge Regression")
# log
Ridge_log_RMSE
Ridge_log_aver_RMSE = mean(Ridge_log_RMSE)
Ridge_log_aver_R2 = mean(Ridge_log_R2)
Ridge_log_aver_MAE = mean(Ridge_log_MAE)
pander(data.frame(R2 = Ridge_log_aver_R2, RMSE = Ridge_log_aver_RMSE, MAE = Ridge_log_aver_MAE ), title="Cross Validation of Ridge Regression After Log Transformation")
# sqrt
Ridge_sqrt_RMSE
Ridge_sqrt_aver_RMSE = mean(Ridge_sqrt_RMSE)
Ridge_sqrt_aver_R2 = mean(Ridge_sqrt_R2)
Ridge_sqrt_aver_MAE = mean(Ridge_sqrt_MAE)
pander(data.frame(R2 = Ridge_sqrt_aver_R2, RMSE = Ridge_sqrt_aver_RMSE, MAE = Ridge_sqrt_aver_MAE), title="Cross Validation of Ridge Regression After Sqrt Transformation")
```

### 5. Lasso Regression

#### Explanation

Lasso regression is pretty similar to Ridge regression. But compare to ridge, Lasso is more interpretable. It will make some predictors' coefficient to be exactly 0, which could help us find out which predictor is what Lasso thinks is important. 

#### Prepare Model

1. Set the initial alpha is equal to 1 (Ridge regression is 0), and also use the same initial lambda, then try to use cross validation to choose the optimal lambda for Lasso.

```{r lasso}
# original 
k=5
Lasso_ori_accuracy= rep(0,k)
Lasso_ori_lambda = rep(0,k)
Lasso_ori_R2 = rep(0,k)
Lasso_ori_MAE = rep(0,k)
Lasso_ori_RMSE = rep(0,k)
Lasso_ori_coef = 0
for ( i in 1:k){
  set.seed(300+i)
  sample = sample(nrow(train_ori),nrow(train_ori),replace = T)
  ori_train = train_ori[sample,]
  train = sample(nrow(ori_train),0.7*nrow(ori_train))
  training_dataset = ori_train[train,]
  validation_dataset = ori_train[-train,]
  X_train = model.matrix(SalePrice~.,data = training_dataset)[,-1]
  X_test = model.matrix(SalePrice~.,validation_dataset)[,-1]
  y_train = training_dataset$SalePrice
  y_test = validation_dataset$SalePrice
  set.seed(1300+i)
  grid=10^seq(10,-2, length =100)
  Lasso.ori.Alpha=1
  Lasso.ori.Fit = glmnet(X_train, y_train, alpha=Lasso.ori.Alpha, lambda=grid)
  Lasso.ori.Fitcv = cv.glmnet(X_train, y_train, alpha = Lasso.ori.Alpha,nfolds = 10,type.measure = 'deviance')
  Lasso.ori.lambda = Lasso.ori.Fitcv$lambda.min
  Lasso_ori_lambda[i] = Lasso.ori.lambda
  Lasso.ori.Pred <- predict(Lasso.ori.Fit, s= Lasso.ori.lambda, newx = X_test)
  Lasso.ori.coef = predict(Lasso.ori.Fit, s = Lasso.ori.lambda, type="coefficients")
  Lasso_ori_coef = Lasso_ori_coef + Lasso.ori.coef 
  Lasso_ori_accuracy[i] = mean(abs((y_test) - (Lasso.ori.Pred))/(y_test) <=0.05)
  Lasso_ori_R2[i] = R2(Lasso.ori.Pred,(y_test))
  Lasso_ori_RMSE[i]= RMSE(Lasso.ori.Pred,(y_test))
  Lasso_ori_MAE[i] = MAE(Lasso.ori.Pred,(y_test))
}  
Lasso_ori_aver_lambda = mean(Lasso_ori_lambda)
Lasso_ori_aver_accuracy = mean(Lasso_ori_accuracy)
Lasso_ori_aver_RMSE = mean(Lasso_ori_RMSE)
Lasso_ori_aver_R2 = mean(Lasso_ori_R2)
Lasso_ori_aver_MAE = mean(Lasso_ori_MAE)
Lasso_ori_aver_coef = (Lasso_ori_coef/5)[1:211,]

# log transformation
k = 5
Lasso_log_accuracy= rep(0,k)
Lasso_log_lambda = rep(0,k)
Lasso_log_R2 = rep(0,k)
Lasso_log_MAE = rep(0,k)
Lasso_log_RMSE = rep(0,k)
Lasso_log_coef = 1
for ( i in 1:k){
  set.seed(340+i)
  sample = sample(nrow(train_log),nrow(train_log),replace = T)
  log_train = train_log[sample,]
  train = sample(nrow(log_train),0.7*nrow(log_train))
  training_dataset = log_train[train,]
  validation_dataset = log_train[-train,]
  X_train = model.matrix(SalePrice~.,data = training_dataset)[,-1]
  X_test = model.matrix(SalePrice~.,validation_dataset)[,-1]
  y_train = training_dataset$SalePrice
  y_test = validation_dataset$SalePrice
  set.seed(1330+i)
  grid=10^seq(10,-2, length =100)
  Lasso.log.Alpha=1
  Lasso.log.Fit = glmnet(X_train, y_train, alpha=Lasso.log.Alpha, lambda=grid)
  Lasso.log.Fitcv = cv.glmnet(X_train, y_train, alpha = Lasso.log.Alpha,nfolds = 10,type.measure = 'deviance')
  Lasso.log.lambda = Lasso.log.Fitcv$lambda.min
  Lasso_log_lambda[i] = Lasso.log.lambda
  Lasso.log.Pred <- predict(Lasso.log.Fit, s= Lasso.log.lambda, newx = X_test)
  Lasso.log.Pred = exp(Lasso.log.Pred)
  Lasso.log.coef = predict(Lasso.log.Fit, s = Lasso.log.lambda, type="coefficients")
  Lasso_log_coef = Lasso_log_coef + Lasso.log.coef
  Lasso_log_accuracy[i] = mean(abs(exp(y_test) - (Lasso.log.Pred))/exp(y_test) <=0.05)
  Lasso_log_R2[i] = R2(Lasso.log.Pred,exp(y_test))
  Lasso_log_RMSE[i]= RMSE(Lasso.log.Pred,exp(y_test))
  Lasso_log_MAE[i] = MAE(Lasso.log.Pred,exp(y_test))
}  
Lasso_log_aver_coef = (Lasso_log_coef/5)[1:211,]
Lasso_log_aver_lambda = mean(Lasso_log_lambda)
Lasso_log_aver_accuracy = mean(Lasso_log_accuracy)
Lasso_log_aver_RMSE = mean(Lasso_log_RMSE)
Lasso_log_aver_R2 = mean(Lasso_log_R2)
Lasso_log_aver_MAE = mean(Lasso_log_MAE)

# sqrt transformation
Lasso_sqrt_accuracy= rep(0,k)
Lasso_sqrt_lambda = rep(0,k)
Lasso_sqrt_R2 = rep(0,k)
Lasso_sqrt_MAE = rep(0,k)
Lasso_sqrt_RMSE = rep(0,k)
Lasso_sqrt_coef = 1
for ( i in 1:k){
  set.seed(360+i)
  sample = sample(nrow(train_sqrt),nrow(train_sqrt),replace = T)
  sqrt_train = train_sqrt[sample,]
  train = sample(nrow(sqrt_train),0.7*nrow(sqrt_train))
  training_dataset = sqrt_train[train,]
  validation_dataset = sqrt_train[-train,]
  X_train = model.matrix(SalePrice~.,data = training_dataset)[,-1]
  X_test = model.matrix(SalePrice~.,validation_dataset)[,-1]
  y_train = training_dataset$SalePrice
  y_test = validation_dataset$SalePrice
  set.seed(1360+i)
  grid=10^seq(10,-2, length =100)
  Lasso.sqrt.Alpha=1
  Lasso.sqrt.Fit = glmnet(X_train, y_train, alpha=Lasso.sqrt.Alpha, lambda=grid)
  Lasso.sqrt.Fitcv = cv.glmnet(X_train, y_train, alpha = Lasso.sqrt.Alpha,nfolds = 10,type.measure = 'deviance')
  Lasso.sqrt.lambda = Lasso.sqrt.Fitcv$lambda.min
  Lasso_sqrt_lambda[i] = Lasso.sqrt.lambda
  Lasso.sqrt.Pred = predict(Lasso.sqrt.Fit, s= Lasso.sqrt.lambda, newx = X_test)
  Lasso.sqrt.Pred = Lasso.sqrt.Pred^4
  Lasso.sqrt.coef = predict(Lasso.sqrt.Fit, s = Lasso.sqrt.lambda, type="coefficients")
  Lasso_sqrt_coef = Lasso_sqrt_coef + Lasso.sqrt.coef 
  Lasso_sqrt_accuracy[i] = mean(abs((y_test)^4 - (Lasso.sqrt.Pred))/(y_test)^4 <=0.05)
  Lasso_sqrt_R2[i] = R2(Lasso.sqrt.Pred,(y_test)^4)
  Lasso_sqrt_RMSE[i]= RMSE(Lasso.sqrt.Pred,(y_test)^4)
  Lasso_sqrt_MAE[i] = MAE(Lasso.sqrt.Pred,(y_test)^4)
}  
Lasso_sqrt_aver_coef = (Lasso_sqrt_coef/5)[1:211,]
Lasso_sqrt_aver_lambda = mean(Lasso_sqrt_lambda)
Lasso_sqrt_aver_accuracy = mean(Lasso_sqrt_accuracy)
Lasso_sqrt_aver_RMSE = mean(Lasso_sqrt_RMSE)
Lasso_sqrt_aver_R2 = mean(Lasso_sqrt_R2)
Lasso_sqrt_aver_MAE = mean(Lasso_sqrt_MAE)
```

1. With cross validation, we find out the optimal lambda as following:

```{r lassoOplam}
# lambda of original 
Lasso_ori_lambda
Lasso_ori_aver_lambda
# lambda of log transformation 
Lasso_log_lambda
Lasso_log_aver_lambda
# lambda of sqrt transformation
Lasso_sqrt_lambda
Lasso_sqrt_aver_lambda

## with cross valdiation, we get the optimal lambda is 
Lasso_ori_aver_lambda
Lasso_log_aver_lambda
Lasso_sqrt_aver_lambda
par(mfrow=c(2,2))
# plot(Lasso.ori.Fitcv)
# plot(Lasso.log.Fitcv)
# plot(Lasso.sqrt.Fitcv)
par(mfrow = c(1,1))
```

#### Coefficient From Lasso Regression

Here we are going to show the predictors lasso choosed.

```{r lassoImp}
# coefficient chose by Lasso
names(head(sort(Lasso_ori_aver_coef[Lasso_ori_aver_coef!=0],decreasing = TRUE),10))

# coefficient chose by Lasso after log transformation 
names(head(sort(Lasso_log_aver_coef[Lasso_log_aver_coef!=0],decreasing = TRUE),10))

# coefficient chose by Lassoafter sqrt transformation 
names(head(sort(Lasso_sqrt_aver_coef[Lasso_sqrt_aver_coef!=0],decreasing = TRUE),10))
```

#### Check accuracy

We then need to check accuracy, as assumed before, we would look at whether the predicted data is within the $\pm 5\%$ range. The following is the result.

```{r lassoAcc}
# original accuracy
Lasso_ori_accuracy
Lasso_ori_aver_accuracy
printf("Accuracy of Lasso is approximately %.2f%%", Lasso_ori_aver_accuracy*100)
# log accuracy
Lasso_log_accuracy
Lasso_log_aver_accuracy = mean(Lasso_log_accuracy)
printf("Accuracy of Lasso with Log Transformation is approximately %.2f%%", Lasso_log_aver_accuracy*100)
# sqrt accuracy
Lasso_sqrt_accuracy
Lasso_sqrt_aver_accuracy
printf("Accuracy of Lasso with Sqrt Transformation is approximately %.2f%%", Lasso_sqrt_aver_accuracy*100)

# accuracy dataframe
Lasso_accuracy_df = data.frame(Lasso = c('accuracy'),ori_accuracy=c(Lasso_ori_aver_accuracy),
                               log_accuracy = c(Lasso_log_aver_accuracy),
                               sqrt_accuracy = c(Lasso_sqrt_aver_accuracy))
```

#### Cross Validation

Then let us take a look at the MSE of this model:

```{r lassoCV}
# ori
Lasso_ori_RMSE
Lasso_ori_aver_RMSE = mean(Lasso_ori_RMSE)
Lasso_ori_aver_R2 = mean(Lasso_ori_R2)
Lasso_ori_aver_MAE = mean(Lasso_ori_MAE)
pander(data.frame(R2 = Lasso_ori_aver_R2,RMSE = Lasso_ori_aver_RMSE,MAE = Lasso_ori_aver_MAE),title="Cross Validation of Lasso Regression")
# log
Lasso_log_RMSE
Lasso_log_aver_RMSE = mean(Lasso_log_RMSE)
Lasso_log_aver_R2 = mean(Lasso_log_R2)
Lasso_log_aver_MAE = mean(Lasso_log_MAE)
pander(data.frame(R2 = Lasso_log_aver_R2, RMSE = Lasso_log_aver_RMSE, MAE = Lasso_log_aver_MAE ), title="Cross Validation of Lasso Regression After Log Transformation")
# sqrt
Lasso_sqrt_RMSE
Lasso_sqrt_aver_RMSE = mean(Lasso_sqrt_RMSE)
Lasso_sqrt_aver_R2 = mean(Lasso_sqrt_R2)
Lasso_sqrt_aver_MAE = mean(Lasso_sqrt_MAE)
pander(data.frame(R2 = Lasso_sqrt_aver_R2, RMSE = Lasso_sqrt_aver_RMSE, MAE = Lasso_sqrt_aver_MAE), title="Cross Validation of Lasso Regression After Sqrt Transformation")

```

### 6. GAM

#### Explanation

We have chosen GAM as one of our models because it produces an analysis on those factors that have less linear relationship with the result, for instance LotFrontage, YearRemodAdd, and MasVnrArea that are having relatively high importance but also high p-value that makes them not very linear related to SalePrice. 

1) GAM1

In this model, we have LotFrontage, YearRemodAdd and MasVnrArea as predictors, with YearRemodAdd having a degree of freedom 2. We obtain the following result:

```{r GAM1, fig.width=5,fig.height=6}
# fit1 = gam(SalePrice ~ s(LotFrontage) + ns(YearRemodAdd,2) + MasVnrArea, data = train_ori)
# printf("Deviance of Model 1 approximately %.2f", deviance(fit1))
# pred1 = predict(fit1, newdata=x_test_ori)
# par(mfrow=c(2,1))
# accuracy = mean(abs(y_test_ori - pred1)/y_test_ori<=0.05)
# printf("Accuracy of Model 1 approximately %.2f%%", accuracy*100)
# plot(fit1 , se=TRUE , col="red")
# 
# fit1_log = gam(SalePrice ~ s(LotFrontage) + ns(YearRemodAdd,2) + MasVnrArea, data = train_log)
# printf("Deviance of Model 1 approximately %.2f", deviance(fit1_log))
# pred1 = predict(fit1, newdata=x_test_log)
# par(mfrow=c(2,1))
# accuracy = mean(abs(y_test_log - pred1)/y_test_log<=0.05)
# printf("Accuracy of Model 1 approximately %.2f%%", accuracy*100)
# plot(fit1_log , se=TRUE , col="red")
# 
# fit1_sqrt = gam(SalePrice ~ s(LotFrontage) + ns(YearRemodAdd,2) + MasVnrArea, data = train_sqrt)
# printf("Deviance of Model 1 approximately %.2f", deviance(fit1_sqrt))
# pred1 = predict(fit1, newdata=x_test_sqrt)
# par(mfrow=c(2,1))
# accuracy = mean(abs(y_test_sqrt - pred1)/y_test_sqrt<=0.05)
# printf("Accuracy of Model 1 approximately %.2f%%", accuracy*100)
# plot(fit1_sqrt , se=TRUE , col="red")
```

2) GAM2

In this model, we have LotFrontage, YearRemodAdd and MasVnrArea as predictors. None of them has a degree of freedom in the fit. We obtain the following result:

```{r GAM2, fig.width=5,fig.height=3}
# fit2 = gam(SalePrice ~ LotFrontage + YearRemodAdd + s(MasVnrArea), data = train_ori)
# printf("Deviance of Model 2 approximately %.2f", deviance(fit2))
# pred2 = predict(fit2, newdata=x_test_ori)
# accuracy = mean(abs(y_test_ori - pred2)/y_test_ori<=0.05)
# printf("Accuracy of Model 2 approximately %.2f%%", accuracy*100)
# plot(fit2 , se=TRUE , col="red")
# 
# fit2_log = gam(SalePrice ~ LotFrontage + YearRemodAdd + s(MasVnrArea), data = train_log)
# printf("Deviance of Model 2 approximately %.2f", deviance(fit2))
# pred2 = predict(fit2, newdata=x_test_log)
# accuracy = mean(abs(y_test_log - pred2)/y_test_log<=0.05)
# printf("Accuracy of Model 2 approximately %.2f%%", accuracy*100)
# plot(fit2_log , se=TRUE , col="red")
# 
# fit2_sqrt = gam(SalePrice ~ LotFrontage + YearRemodAdd + s(MasVnrArea), data = train_sqrt)
# printf("Deviance of Model 2 approximately %.2f", deviance(fit2))
# pred2 = predict(fit2, newdata=x_test_sqrt)
# accuracy = mean(abs(y_test_sqrt - pred2)/y_test_sqrt<=0.05)
# printf("Accuracy of Model 2 approximately %.2f%%", accuracy*100)
# plot(fit2_sqrt , se=TRUE , col="red")
```

3) GAM3

In this model, we have LotFrontage, YearRemodAdd and MasVnrArea as predictorswith LotFrontage having a degree of freedom of 3. We obtain the following result:

```{r GAM3, fig.width=5,fig.height=3}
# fit3 = gam(SalePrice ~ ns(LotFrontage,3) + YearRemodAdd + s(MasVnrArea), data = train_ori)
# printf("Deviance of Model 3 approximately %.2f", deviance(fit3))
# pred3 = predict(fit3, newdata=x_test_ori)
# accuracy = mean(abs(y_test_ori - pred3)/y_test_ori<=0.05)
# printf("Accuracy of Model 3 approximately %.2f%%", accuracy*100)
# plot(fit3 , se=TRUE , col="red")
# 
# fit3_log = gam(SalePrice ~ ns(LotFrontage,3) + YearRemodAdd + s(MasVnrArea), data = train_log)
# printf("Deviance of Model 3 approximately %.2f", deviance(fit3_log))
# pred3 = predict(fit3_log, newdata=x_test_log)
# accuracy = mean(abs(y_test_log - pred3)/y_test_log<=0.05)
# printf("Accuracy of Model 3 approximately %.2f%%", accuracy*100)
# plot(fit3_log, se=TRUE , col="red")
# 
# fit3_sqrt = gam(SalePrice ~ ns(LotFrontage,3) + YearRemodAdd + s(MasVnrArea), data = train_sqrt)
# printf("Deviance of Model 3 approximately %.2f", deviance(fit3))
# pred3 = predict(fit3_sqrt, newdata=x_test_sqrt)
# accuracy = mean(abs(y_test_sqrt - pred3)/y_test_sqrt<=0.05)
# printf("Accuracy of Model 3 approximately %.2f%%", accuracy*100)
# plot(fit3_sqrt , se=TRUE , col="red")
```

GAM Summary

We then take an ANOVA test to understand which model is the best and we have the following result:

```{r GAMSummary}
# anova(fit1,fit2,fit3,test="F")
# anova(fit1_log,fit2_log,fit3_log,test="F")
# anova(fit1_sqrt,fit2_sqrt,fit3_sqrt,test="F")
```
We can see that from the anova test that P-value for the second model is the smallest, therefore, it is the most preferred.  

#### Cross Validation

Then, we conduct a cross-validation on the second model only. 

```{r GAMCV}
# set.seed(123) 
# 
# pander(data.frame( R2 = R2(pred2, y_test_ori), 
#             RMSE = RMSE(pred2, y_test_ori), 
#             MAE = MAE(pred2, y_test_ori)), title="Cross Validation of Model 2")
```





# Evaluation of different models

Root MSE

# Choose best fit model

# Conclusion
1. Classfication 

# Discussion & Future Development

# Resources






















































