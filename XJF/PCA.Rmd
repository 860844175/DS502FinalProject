---
title: "House Pricing Prediction"
subtitle: "DS502 Final Project"
author: "Yufei Lin, Jingfeng Xia, Jinhong Yu, Shijing Yang, Yanze Wang"
date: "Nov 29 2020"
output:
  pdf_document: default
  word_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
# check R version
R.Version()$major

# set up document
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(fig.pos = "H", out.extra = "")
knitr::opts_chunk$set(fig.width=5,fig.height=3)
library(pander)
library(knitr)
library(skimr)
library(kableExtra)
library(tinytex)
library(dplyr)
library(purrr)
local({
  hook_inline = knitr::knit_hooks$get('inline')
  knitr::knit_hooks$set(inline = function(x) {
    res = hook_inline(x)
    if (is.numeric(x)) sprintf('$%s$', res) else res
  })
})

# define printf function
printf <- function(...)print(sprintf(...))
```
```{r oriTestTrainReg, include=FALSE}
test_ori = read.csv("../SourceData/test_original.csv")
y_test_ori = test_ori$SalePrice
x_test_ori = subset (test_ori, select = -SalePrice)
# x_test_ori = x_test_ori[,0:-5]
train_ori = read.csv("../SourceData/train_original.csv")
y_train_ori = train_ori$SalePrice
x_train_ori = subset (train_ori, select = -SalePrice)
y_train_ori = as.numeric(y_train_ori)
y_test_ori = as.numeric(y_test_ori)
summary(y_train_ori)
```

### 2. PCA (Jeff)
In order to reduce the dimension convincing by finding a proper Manifold, we apply PCA method. Fortunately, we successfully reduced the dimension from 216 to no more than 32. 32 PCs have totally 99.997% of variance proportion. 29 PCs are enough for totally getting 85% of variance proportion.

```{r PCA}
x_train_ori.pr = prcomp(x_train_ori)
print(summary(x_train_ori.pr))
```

In the summary we can find all variance proportions behind 32th PC are 0, which means 32 PCs are totally enough important to describe all features of the original data. In fact, we can take less than 32 PCs and
find the best number of PCs with cross validation in training process.
The sum of (0.1627, 0.1073, 0.105, 0.07973, 0.05255, 0.05155, 0.03586, 0.03352, 0.03141, 0.02978, 0.02679, 0.02574, 0.02376, 0.02327, 0.02185, 0.02102, 0.01885, 0.01826, 0.01709, 0.01591, 0.01533, 0.01396, 0.01287, 0.0110, 0.01046, 0.00921, 0.00687, 0.00639, 0.00596, 0.00374, 0.00187, 0.00037) is  0.99997, which means 32 PCs totally have 99.997% of variance proportion. Moreover, 29 PCs are enough for totally getting 85% of variance proportion.
```{r pcaplot1}
plot(x_train_ori.pr,type = "l", main = NULL)
```

In this plot, you can see an elbow at 7. Perhaps it can be a good number of PCs.

```{r pcaplot2}
y = list(0.1627, 0.1073, 0.105, 0.07973, 0.05255, 0.05155, 0.03586, 0.03352, 0.03141, 0.02978, 0.02679, 0.02574, 0.02376, 0.02327, 0.02185, 0.02102, 0.01885, 0.01826, 0.01709, 0.01591, 0.01533, 0.01396, 0.01287, 0.0110, 0.01046, 0.00921, 0.00687, 0.00639, 0.00596, 0.00374, 0.00187, 0.00037)
x = c(1:32)
plot(x,y, type="l")
```

This is generated by plotting all variance proportions of 32 PCs. We can see the elbow at 7 more clearly than the previous one. However, the square of tail on the right side of 7 is quite thick, which means maybe a number on the tail can be the best one for training. So the best number of PC is between 7 and 32. Last but not the least, the exact best number can only be revealed by cross validation.

```{r newdata}
# New data list cleansed by PCA size of 868*32, which can take the place of original data. 
# Each column is one PC, PC's importance decreases by column number.
# Try CV on number of PC you drop from right to left and know how many PCs are best to use.
newdata = x_train_ori.pr$x[,1:32]
print(nrow(x_train_ori))
print(ncol(x_train_ori))
print(nrow(newdata))
print(ncol(newdata))
# print(newdata[,1]) # 1st PCA, the most important PC
# print(newdata[,32]) # 32th PCA, the least important PC
```

This is to show the effect of dimension reduction and how can we take and use the new data modified by PCA method.