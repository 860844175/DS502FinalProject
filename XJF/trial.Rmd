---
title: "House Pricing Prediction"
subtitle: "DS502 Final Project"
author: "Yufei Lin, Jingfeng Xia, Jinhong Yu, Shijing Yang, Yanze Wang"
date: "Nov 29 2020"
output:
  pdf_document: default
  word_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
# check R version
R.Version()$major

# set up document
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(fig.pos = "H", out.extra = "")
knitr::opts_chunk$set(fig.width=5,fig.height=3)
library(pander)
library(knitr)
library(skimr)
library(kableExtra)
library(tinytex)
library(dplyr)
library(purrr)
local({
  hook_inline = knitr::knit_hooks$get('inline')
  knitr::knit_hooks$set(inline = function(x) {
    res = hook_inline(x)
    if (is.numeric(x)) sprintf('$%s$', res) else res
  })
})

# define printf function
printf <- function(...)print(sprintf(...))
```

```{r libraries, include=FALSE}
# Import model libraries
library(pls)
library(randomForest)
library(gam)
library(glmnet)
library(ggplot2)
library(corrplot)
library(tidyverse)
library(caret)
library(mgcv)
library(Metrics)
library(visreg)
library(boot)
library('ggthemes') 
library('scales')
library('mice')
library('data.table')
library('gridExtra') 
library('GGally')
library('e1071')
```

# Introduction

## Description of the Problem

Being able to predict the price of a house tends to be an important skill for both the seller and consumers. For the seller, they could make better sales and consumers could have better understanding when they try to make a purchase. Therefore, in this project, we are planning to make prediction of house price based on the 79 different predictors provided by Kaggle dataset to determine values of residential homes in Ames, Iowa. We have noticed Sale Price has a typical right-skewed distribution, and decided to process it using two ways, logrithmic with base $e$ and square root for a distribution that is much closer to the shape of a Gaussian distribution for better performance in models like linear regression. In this analysis, we will perform random forest on original y-value for importance of variables and all the rest of models on both absolute and processed y-values to see which way would each model do better and provide an ensemble of models at the end of our study. 

## Description of the Dataset

In terms of the dataset, the entire data set consists of two pieces of data organized as training data set and test data set respectively. Whereas for each of the dataset, approximately 80 columns corresponding parameters would be evaluated with the prediction of house price. Some noteworthy predictors include the location classification, utilities, environment of neighborhood, house style and condition, area, year of built, and number of functioning rooms. There are over 1400 row of data points in both the training data set and the test data set. The sale prices in the train dataset are given as a parameter in the form of five or six figure full flat integers. The test data set will be applied to different regression models in order to distinguish the disparities of different model performances. 
    
## Approaches

Given that our data is aimed at predicting Sale Price of a house, it is unreasonable to require a model to fit the exact value of the dataset but only to reach an estimation within a certain range. Therefore, we have decided to use both regression and classification approaches to look at the problem on both the original and processed value. For regression method, we are going to look at if a prediction is within the range of the actual price $\pm 5\%$, we will say it is an accurate prediction. For classification prediction, we will be tagging the data into several different groups, and would be fitting the threshold accordingly with models like SVM and K-Means clustering. 

# Data Processing

## Read in Data

We have chosen to eliminate the Id column from this dataset because Id has nothing to do with our prediction and would mess up our prediction. We save data in "train.csv"" from Kaggle into a variable named \textbf{HousePricing} for further processing and we will separate it into training and testing set. For each model Bootstrapping will be performed before each model's training process.   

```{r readData}
HousePricing = read.csv("./SourceData/train_new.csv")
HousePricing = subset(HousePricing,select=-Id)
```

## Data Exploration

```{r dimensionOfData}
# The number of columns and rows
paste("Original training data set has",dim(HousePricing)[1], "rows and", dim(HousePricing)[2], "columns")

# The percentage of data missing in train
paste("The percentage of data missing in the original training data set is ", round(sum(is.na(HousePricing)) / (nrow(HousePricing) *ncol(HousePricing)),4)*100,"%",sep = "")

# The number of duplicated rows
paste("The number of duplicated rows are", nrow(HousePricing) - nrow(unique(HousePricing)))
```

```{r numOfNumericAndFactors}
print("Number of Factors:")
sum(sapply(HousePricing[,1:80],typeof) == "character")
print("Number of Numeric:")
sum(sapply(HousePricing[,1:80],typeof) == "integer")
```
```{r summaryOfData}
pander(summary(HousePricing[,sapply(HousePricing[,1:80],typeof) == "integer"]))
```



```{r visualization}
# data visualization
cat.var = names(HousePricing)[which(sapply(HousePricing, is.character))]
num.var = names(HousePricing)[which(sapply(HousePricing, is.numeric))]
train.num = HousePricing[num.var]
train.cat = HousePricing[cat.var]

## Bar plot/Density plot function

## Bar plot function

plotHist <- function(data_in, i) 
{
  data <- data.frame(x=data_in[[i]])
  p <- ggplot(data=data, aes(x=factor(x))) + stat_count() + xlab(colnames(data_in)[i]) + theme_light() + 
    theme(axis.text.x = element_text(angle = 90, hjust =1))
  return (p)
}

## Density plot function

plotDen <- function(data_in, i){
  data <- data.frame(x=data_in[[i]], SalePrice = data_in$SalePrice)
  p <- ggplot(data= data) + geom_line(aes(x = x), stat = 'density', size = 1,alpha = 1.0) +
    xlab(paste0((colnames(data_in)[i]), '\n', 'Skewness: ',round(skewness(data_in[[i]], na.rm = TRUE), 2))) + theme_light() 
  return(p)
  
}

## Function to call both Bar plot and Density plot function

doPlots <- function(data_in, fun, ii, ncol=3) 
{
  pp <- list()
  for (i in ii) {
    p <- fun(data_in=data_in, i=i)
    pp <- c(pp, list(p))
  }
  do.call("grid.arrange", c(pp, ncol=ncol))
}

```

```{r plotting}
# barplots for categorical features
doPlots(train.cat, fun = plotHist, ii = 1:4, ncol = 2)

doPlots(train.cat, fun = plotHist, ii = 5:8, ncol = 2)

doPlots(train.cat, fun = plotHist, ii = 9:12, ncol = 2)

doPlots(train.cat, fun = plotHist, ii = 13:18, ncol = 2)

doPlots(train.cat, fun = plotHist, ii = 19:22, ncol = 2)
```

```{r plot}

# boxplots

ggplot(HousePricing, aes(x = Neighborhood, y = SalePrice)) +
  geom_boxplot() +
  geom_hline(aes(yintercept=80), 
             colour='red', linetype='dashed', lwd=2) +
  scale_y_continuous(labels=dollar_format()) +
  theme_few()

# conclusion: boxplot between the neighboorhoods and sale price shows that BrookSide and 
# South & West of Iowa State University have cheap houses. 
# While Northridge and Northridge Heights are rich neighborhoods with several outliers in terms of price.

# density plots for numeric variables
doPlots(train.num, fun = plotDen, ii = 2:6, ncol = 2)
doPlots(train.num, fun = plotDen, ii = 7:12, ncol = 2)
doPlots(train.num, fun = plotDen, ii = 13:17, ncol = 2)

# Conclusion: Density plots of the features indicates that the features are skewed. 
# The denisty plot for YearBuilt shows that the data set contains a mix of new and old houses. 
# It shows a downturn in the number of houses in recent years, possibily due to the housing crisis.

doPlots(train.num, fun = plotHist, ii = 18:23, ncol = 2)

# Conclusion: The histograms below show that majority of the houses have 2 full baths, 0 half baths, 
# and have an average of 3 bedrooms.

# Explore correlation

correlation = cor(na.omit(train.num))
row_indic = apply(correlation, 1, function(x) sum(x > 0.3 | x < -0.3) > 1)
correlation = correlation[row_indic,row_indic]
corrplot(correlation,method = "shade")

```

### target varaible vs. predictors

```{r targetvspredictors}
summary(HousePricing$SalePrice)
quantile(HousePricing$SalePrice)
hist(HousePricing$SalePrice,col="blue",breaks = 25,main = "Distribution of SalePrice", xlab = "Sale Price")
```

\textbf{Conclusion}

It deviates from normal distribution and it is right skewed

### Plotting 'GrLivArea' too see if there are any outliers

```{r outlier}
qplot(GrLivArea, SalePrice, data= HousePricing,col=GrLivArea>4000,xlab = "GrLivArea", ylab="Sale Price",main = "Living Area vs. Sale Price")

summary(HousePricing$GrLivArea)
hist(HousePricing$GrLivArea,breaks = 20,xlab="Living area",col = "dark red",main = "Frequency of Living area square feet")
```

## Feature Engineering

In this section, we convert all missing value based on the following rules:

\begin{enumerate}
\item Categorical: fill in most common
\item Numeric: fill in median/average
\end{enumerate}

Convert all train to HousePricing

```{r simplePlot}
ggplot(HousePricing,aes(x=GrLivArea,y=SalePrice))+geom_point()
```
```{r featuerEngineering, include=FALSE}
# remove ID column
HousePricing$Id = NULL
HousePricing = HousePricing[HousePricing$GrLivArea<4500,]

# for using later
numericVars <- which(sapply(HousePricing, is.numeric))
numericVarNames <- names(numericVars) 

# find the columns contains NA and the number of NA values in each columns
NAcol <- which(colSums(is.na(HousePricing)) > 0)
sort(colSums(sapply(HousePricing[NAcol], is.na)), decreasing = TRUE)

# LotFrontage
# compute the median of neighbor, na.rm means compute medians without NA
neighbor_Median  = HousePricing %>%
  select(LotFrontage, Neighborhood) %>%
  group_by(Neighborhood) %>%
  summarise(LotFrontage = median(LotFrontage, na.rm = T))

# replace the LotFrontage NA with its neighbor's Lotfrontage's median.
for (i in 1:nrow(HousePricing))
{
  if(is.na(HousePricing$LotFrontage[i])){
    temp = HousePricing$Neighborhood[i]
    HousePricing$LotFrontage[i] = neighbor_Median$LotFrontage[neighbor_Median$Neighborhood == temp]
  }
}

# Alley, NA means no alley.
HousePricing$Alley[is.na(HousePricing$Alley)] = 'None'
HousePricing$Alley = as.factor(HousePricing$Alley)

HousePricing$Utilities = NULL
table(HousePricing$PoolQC)
HousePricing$PoolQC[is.na(HousePricing$PoolQC)] = "None"
HousePricing$PoolQC=recode(HousePricing$PoolQC,'None' = 0,'Fa' = 1,'TA' = 2,'Gd' = 3,'Ex' = 4)
# Fence
HousePricing$Fence[is.na(HousePricing$Fence)] = "None"
HousePricing$Fence = as.factor(HousePricing$Fence)
HousePricing$MiscFeature[is.na(HousePricing$MiscFeature)] = "None"
HousePricing$MiscFeature = as.factor(HousePricing$MiscFeature)

# garage
HousePricing$GarageYrBlt[is.na(HousePricing$GarageYrBlt)] <- HousePricing$YearBuilt[is.na(HousePricing$GarageYrBlt)]
HousePricing$GarageType[is.na(HousePricing$GarageType)] = "None"
HousePricing$GarageType = as.factor(HousePricing$GarageType)
HousePricing$GarageFinish[is.na(HousePricing$GarageFinish)] = "None"
HousePricing$GarageFinish=recode(HousePricing$GarageFinish,'None' = 0,'Unf' = 1,'RFn' = 2,'Fin' = 3)
HousePricing$GarageQual[is.na(HousePricing$GarageQual)] = "None"
HousePricing$GarageQual=recode(HousePricing$GarageQual,'None' = 0,'Po' = 1,'Fa' = 2,'TA' = 3,'Gd' = 4,'Ex' = 5)
HousePricing$GarageCond[is.na(HousePricing$GarageCond)] = "None"
HousePricing$GarageCond=recode(HousePricing$GarageCond,'None' = 0,'Po' = 1,'Fa' = 2,'TA' = 3,'Gd' = 4,'Ex' = 5)
HousePricing$FireplaceQu[is.na(HousePricing$FireplaceQu)] = "None"
HousePricing$FireplaceQu=recode(HousePricing$FireplaceQu,'None' = 0,'Po' = 1,'Fa' = 2,'TA' = 3,'Gd' = 4,'Ex' = 5)

#electric
HousePricing$Electrical[is.na(HousePricing$Electrical)] = "SBrkr"
HousePricing$Electrical = as.factor(HousePricing$Electrical)

# basement 
length(which(is.na(HousePricing$BsmtQual) & is.na(HousePricing$BsmtCond) & is.na(HousePricing$BsmtExposure) & is.na(HousePricing$BsmtFinType1) & is.na(HousePricing$BsmtFinType2)))
HousePricing[!is.na(HousePricing$BsmtCond) & (is.na(HousePricing$BsmtFinType1)|is.na(HousePricing$BsmtQual)|is.na(HousePricing$BsmtExposure)|is.na(HousePricing$BsmtFinType2)), c('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2')]
HousePricing$BsmtFinType2[333] = names(sort(table(HousePricing$BsmtFinType2),decreasing = TRUE))[1]
HousePricing$BsmtExposure[949] = names(sort(table(HousePricing$BsmtExposure),decreasing = TRUE))[1]
HousePricing$BsmtExposure[is.na(HousePricing$BsmtExposure)] = 'None'
HousePricing$BsmtExposure=recode(HousePricing$BsmtExposure,'None' = 0,'No' = 1,'Mn' = 2,'Av' = 3,'Gd' = 4)
HousePricing$BsmtQual[is.na(HousePricing$BsmtQual)] = 'None'
HousePricing$BsmtQual=recode(HousePricing$BsmtQual,'None' = 0,'Po' = 1,'Fa' = 2,'TA' = 3,'Gd' = 4,'Ex' = 5)
HousePricing$BsmtCond[is.na(HousePricing$BsmtCond)] = 'None'
HousePricing$BsmtCond=recode(HousePricing$BsmtCond,'None' = 0,'Po' = 1,'Fa' = 2,'TA' = 3,'Gd' = 4,'Ex' = 5)
HousePricing$BsmtFinType1[is.na(HousePricing$BsmtFinType1)] = 'None'
HousePricing$BsmtFinType1=recode(HousePricing$BsmtFinType1,'None' = 0,'Unf' = 1,'LwQ' = 2,'Rec' = 3,'BLQ' = 4,'ALQ' = 5, 'GLQ' = 6)
HousePricing$BsmtFinType2[is.na(HousePricing$BsmtFinType2)] = 'None'
HousePricing$BsmtFinType2=recode(HousePricing$BsmtFinType2,'None' = 0,'Unf' = 1,'LwQ' = 2,'Rec' = 3,'BLQ' = 4,'ALQ' = 5, 'GLQ' = 6)

# Mas
HousePricing$MasVnrType[is.na(HousePricing$MasVnrType)] = 'None'
HousePricing$MasVnrType = as.factor(HousePricing$MasVnrType)
HousePricing$MasVnrArea[(is.na(HousePricing$MasVnrArea))] = 0

HousePricing$MSZoning = as.factor(HousePricing$MSZoning)
HousePricing$Street = as.factor(HousePricing$Street)
HousePricing$LotShape=recode(HousePricing$LotShape,'IR3' = 0,'IR2' = 1,'IR1' = 2,'Reg' =2)
HousePricing$LotConfig = as.factor(HousePricing$LotConfig)

HousePricing$Condition1 = as.factor(HousePricing$Condition1)
HousePricing$Condition2 = as.factor(HousePricing$Condition2)
HousePricing$LandContour = as.factor(HousePricing$LandContour)
HousePricing$RoofStyle = as.factor(HousePricing$RoofStyle)
HousePricing$LandSlope=recode(HousePricing$LandSlope,'Sev' = 0,'Mod' = 1,'Gtl' = 2)

HousePricing$BldgType = as.factor(HousePricing$BldgType)
HousePricing$HouseStyle=as.factor(HousePricing$HouseStyle)


HousePricing$RoofMatl=as.factor(HousePricing$RoofMatl)
HousePricing$Exterior1st=as.factor(HousePricing$Exterior1st)
HousePricing$Exterior2nd=as.factor(HousePricing$Exterior2nd)
HousePricing$ExterQual=recode(HousePricing$ExterQual,'Po' = 0,'Fa' = 1,'TA' = 2,'Gd' = 3,'Ex' = 4)
HousePricing$ExterCond=recode(HousePricing$ExterCond,'Po' = 0,'Fa' = 1,'TA' = 2,'Gd' = 3,'Ex' = 4)

HousePricing$Foundation = as.factor(HousePricing$Foundation)
HousePricing$PavedDrive=recode(HousePricing$PavedDrive,'N' = 0,'P' = 1,'Y' = 2)
HousePricing$Heating = as.factor(HousePricing$Heating)
HousePricing$HeatingQC=recode(HousePricing$HeatingQC,'Po' = 0,'Fa' = 1,'TA' = 2,'Gd' = 3,'Ex' = 4)
HousePricing$CentralAir=recode(HousePricing$CentralAir,'N' = 0,'Y' = 1)


HousePricing$KitchenQual=recode(HousePricing$KitchenQua,'Po' = 0,'Fa' = 1,'TA' = 2,'Gd' = 3,'Ex' = 4)
HousePricing$Functional=recode(HousePricing$Functional,'Sal' = 0,'Sev' = 1,'Maj2' = 2,'Maj1' = 3,'Mod' = 4,'Min2' = 5,'Min1' = 6,'Typ' = 7)
HousePricing$Neighborhood = as.factor(HousePricing$Neighborhood)
HousePricing$SaleType = as.factor(HousePricing$SaleType)
HousePricing$SaleCondition = as.factor(HousePricing$SaleCondition)
HousePricing$MoSold = NULL
HousePricing$MSSubClass = as.factor(HousePricing$MSSubClass)
HousePricing$MSSubClass=recode(HousePricing$MSSubClass,'20' = '1-STORY 1946+',
                               '30' = '1-STORY 1945-','40' = '1-STORY Unf Attic',
                               '45' = "1/2 STORY Unf Attic",'50' = '1/2 STORY Fin',
                               '60' = '2-STORY+','70' = '2-STORY 1945-','80' = 'SPLIT OR MULTI-LEVEL',
                               '85' = 'SPLIT FOYER','90' = 'DUPLEX', '120' = '1-STORY PUD 1946+',
                               '150' = '1/2 STORY PUD','160' = '2-STORY PUD 1946+',
                               '180' = 'PUD - MULTILEVEL',' 190' = '2 FAMILY CONVERSION')




```

Correlation between the numerical variables

```{r corNum}
# draw a plot of correlation between numerical variables in original data
#which(sapply(HousePricing, is.numeric))
numericVars = which(sapply(HousePricing, is.numeric))#numericVars
factorVars = which(sapply(HousePricing, is.factor))
numVar = HousePricing[,numericVars]
cor_numVar =(cor(numVar))
cor_sorted = as.matrix(sort(cor_numVar[,"SalePrice"],decreasing = TRUE))
#cor_sorted
CorHigh <- names(which(apply(cor_sorted, 1, function(x) abs(x)>0.5)))
cor_numVar <- cor_numVar[CorHigh, CorHigh]
corrplot.mixed(cor_numVar, tl.col="black", tl.pos = "lt", tl.cex = 0.7,cl.cex = .7, number.cex=.7)

# draw a importance plot of all predictors in the original data
set.seed(2018)
quick_RF <- randomForest(x=HousePricing[,-78], y=HousePricing$SalePrice, ntree=100,importance=TRUE)
imp_RF <- importance(quick_RF)
imp_DF <- data.frame(Variables = row.names(imp_RF), MSE = imp_RF[,1])
imp_DF <- imp_DF[order(imp_DF$MSE, decreasing = TRUE),]
ggplot(imp_DF[1:15,], aes(x=reorder(Variables, MSE), y=MSE, fill=MSE)) + geom_bar(stat = 'identity') + labs(x = 'Variables', y= '% increase MSE if variable is randomly permuted') + coord_flip() + theme(legend.position="none")

```
```{r ffe}
########################### further feature engineer ###################
# whether remod 
HousePricing$Remod = ifelse(HousePricing$YearBuilt == HousePricing$YearRemodAdd,0,1)
# the age of house 
HousePricing$Age = as.numeric(HousePricing$YrSold) - HousePricing$YearRemodAdd 
# whether is new
HousePricing$isnew = ifelse(HousePricing$YrSold == HousePricing$YearBuilt,1,0)
# total area.
HousePricing$TotalSqFeet = HousePricing$GrLivArea+HousePricing$TotalBsmtSF
# count the totol number of bathroom in the hourse
HousePricing$TotBathrooms <- HousePricing$FullBath + (HousePricing$HalfBath*0.5) + HousePricing$BsmtFullBath + (HousePricing$BsmtHalfBath*0.5)

HousePricing$TotalPorchSF <- HousePricing$OpenPorchSF + HousePricing$EnclosedPorch + HousePricing$X3SsnPorch + HousePricing$ScreenPorch

# draw a correlation plot after combine some variable
numericVars = which(sapply(HousePricing, is.numeric))#numericVars
factorVars = which(sapply(HousePricing, is.factor))
numVar = HousePricing[,numericVars]
cor_numVar =(cor(numVar))
cor_sorted = as.matrix(sort(cor_numVar[,"SalePrice"],decreasing = TRUE))
#cor_sorted
CorHigh <- names(which(apply(cor_sorted, 1, function(x) abs(x)>0.5)))
cor_numVar <- cor_numVar[CorHigh, CorHigh]
corrplot.mixed(cor_numVar, tl.col="black", tl.pos = "lt", tl.cex = 0.7,cl.cex = .7, number.cex=.7)

# choose some true numerical variable to normalize(not include the encoded part)
numericVarNames <- numericVarNames[!(numericVarNames %in% c('MSSubClass', 'MoSold', 'YrSold', 'SalePrice', 'OverallQual', 'OverallCond'))]
numericVarNames <- append(numericVarNames, c('Age', 'TotalPorchSF', 'TotBathrooms', 'TotalSqFeet'))

# delete some highly correlated variables
HousePricing = subset(HousePricing, select = -c(GrLivArea,ExterQual,GarageArea,X1stFlrSF,
                                           TotRmsAbvGrd,TotalBsmtSF,GarageYrBlt,FullBath,
                                           HalfBath,YearRemodAdd,BsmtHalfBath,BsmtFullBath))
DFnumeric <- HousePricing[, names(HousePricing) %in% numericVarNames]
DFfactors <- HousePricing[, !(names(HousePricing) %in% numericVarNames)]
DFfactors <- DFfactors[, names(DFfactors) != 'SalePrice']

########################## Normalizing the numerical data #########################
predf = scale(DFnumeric,center = T,scale = T)

```

As discussed before, we have decided to use logrithmic with base $e$ and square root to process the data. We have also saved $15\%$ of our data into a variable named vault for the final test of each model. 

```{r getFinalData}
############ one-hot encoding and combine with scaled numerical data #######################################
dfdummies = model.matrix(~.-1,DFfactors) %>% as.data.frame()
newdata = cbind(predf,dfdummies)
newdata$SalePrice = HousePricing$SalePrice

set.seed(1)
vault = sample(1:nrow(newdata), nrow(newdata)*0.15)
dVault = newdata[vault,]
newdata = newdata[-vault,]
oriHouseP = newdata
oriHouseP$SalePrice = newdata$SalePrice
sumOri = summary(oriHouseP$SalePrice)
sumOri
logHouseP = newdata
logHouseP$SalePrice = log(newdata$SalePrice)
sumLog = summary(logHouseP$SalePrice)
hist(logHouseP$SalePrice)
sumLog
sqrtHouseP = newdata
sqrtHouseP$SalePrice = '^'(newdata$SalePrice,1/4)
sumSqrt = summary(sqrtHouseP$SalePrice)
hist(sqrtHouseP$SalePrice)
sumSqrt
```

```{r toFactor}
toFac <- function(original){
  result = original
  result$SalePrice[which(result$SalePrice<=summary(original$SalePrice)["1st Qu."])] = 0
  result$SalePrice[which(result$SalePrice>=summary(original$SalePrice)["3rd Qu."])] = 2
  result$SalePrice[which(result$SalePrice<summary(original$SalePrice)["3rd Qu."] & result$SalePrice>summary(original$SalePrice)["1st Qu."])] = 1
  result$SalePrice = as.factor(result$SalePrice)
  print(summary(result$SalePrice))
  return(result)
}

oriHousePCl = toFac(oriHouseP)
logHousePCl = toFac(logHouseP)
sqrtHousePCl = toFac(sqrtHouseP)
```


```{r prepareNewData, fig.width=5,fig.height=3}
# numvar = which(sapply(combined_data, is.numeric))
# catvar = which(sapply(combined_data, is.factor))
# numdata = combined_data[,numvar]
# numcor =(cor(numdata))
# corsorted = as.matrix(sort(numcor[,"SalePrice"],decreasing = TRUE))
# CorHigh <- names(which(apply(corsorted, 1, function(x) abs(x)>0.5)))
# numcor <- numcor[CorHigh, CorHigh]
# corrplot.mixed(numcor, tl.col="black", tl.pos = "lt", tl.cex = 0.7,cl.cex = .7, number.cex=.7)
# temp = subset(combined_data,select=-SalePrice)
# # standardize numerical data 
# numeric = select_if(temp,is.numeric)
# stnumer = scale(numeric,center = T,scale = T)
# convFact = select_if(temp,is.factor)
# # one hot
# convFact = model.matrix(~.-1,convFact) %>% data.frame()
# SalePrice = log(combined_data$SalePrice)
# # put standardized numerical data and categorical data in one data?
# newdata = cbind(stnumer,convFact,SalePrice)
```

```{r bootstrapingFunc, include=FALSE}
bsF<- function(datadf, randomizer){
  set.seed(randomizer)
  sample = sample(dim(datadf)[1],dim(datadf)[1],replace = T)
  btnewdata = datadf[sample,]
  return(btnewdata)
}
newOriHouseP = bsF(oriHouseP, 1234)
newLogHouseP = bsF(logHouseP, 1234)
newSqrtHouseP = bsF(sqrtHouseP, 1234)
newOriHousePCl = bsF(oriHousePCl, 1234)
newLogHousePCl = bsF(logHousePCl, 1234)
newSqrtHousePCl = bsF(sqrtHousePCl, 1234)
```


## Seperate into Test and Training Set

Spearate by 70% train, 30% test. 

```{r saveToCSV}
toCsv <- function(df, fileName){
  set.seed(10)
  randS = sample(1:nrow(df), nrow(df)*0.7)
  train = df[randS,]
  test = df[-randS,]
  write.csv(train,paste("./SourceData/train_",fileName, ".csv",sep=""), row.names = FALSE)
  write.csv(test,paste("./SourceData/test_",fileName, ".csv",sep=""), row.names = FALSE)
}

toCsv(oriHouseP, "original")
toCsv(logHouseP, "log")
toCsv(sqrtHouseP, "sqrt")
```



```{r oriTestTrainReg, include=FALSE}
test_ori = read.csv("./SourceData/test_original.csv")
y_test_ori = test_ori$SalePrice
x_test_ori = subset (test_ori, select = -SalePrice)
train_ori = read.csv("./SourceData/train_original.csv")
y_train_ori = train_ori$SalePrice
x_train_ori = subset (train_ori, select = -SalePrice)
y_train_ori = as.numeric(y_train_ori)
y_test_ori = as.numeric(y_test_ori)
summary(y_train_ori)
```

```{r logTestTrainReg, include=FALSE}
test_log = read.csv("./SourceData/test_log.csv")
y_test_log = test_ori$SalePrice
x_test_log = subset (test_log, select = -SalePrice)
train_log = read.csv("./SourceData/train_log.csv")
y_train_log = train_log$SalePrice
x_train_log = subset (train_log, select = -SalePrice)
y_train_log = as.numeric(y_train_log)
y_test_log = as.numeric(y_test_log)
summary(y_train_log)
```

```{r sqrtTestTrainReg, include=FALSE}
test_sqrt = read.csv("./SourceData/test_sqrt.csv")
y_test_sqrt = test_sqrt$SalePrice
x_test_sqrt = subset (test_sqrt, select = -SalePrice)
train_sqrt = read.csv("./SourceData/train_sqrt.csv")
y_train_sqrt = train_sqrt$SalePrice
x_train_sqrt = subset (train_sqrt, select = -SalePrice)
y_train_sqrt = as.numeric(y_train_sqrt)
y_test_sqrt = as.numeric(y_test_sqrt)
summary(y_train_sqrt)
```

```{r oriTestTrainCl, include=FALSE}
test_ori_cl = read.csv("./SourceData/test_original_cl.csv")
y_test_ori_cl = test_ori_cl$SalePrice
x_test_ori_cl = subset (test_ori_cl, select = -SalePrice)
train_ori_cl = read.csv("./SourceData/train_original_cl.csv")
y_train_ori_cl = train_ori_cl$SalePrice
x_train_ori_cl = subset (train_ori_cl, select = -SalePrice)
y_train_ori_cl = as.factor(y_train_ori_cl)
y_test_ori_cl = as.factor(y_test_ori_cl)
summary(y_train_ori_cl)
```

```{r logTestTrainCl, include=FALSE}
test_log_cl = read.csv("./SourceData/test_original_cl.csv")
y_test_log_cl = test_log_cl$SalePrice
x_test_log_cl = subset (test_log_cl, select = -SalePrice)
train_log_cl = read.csv("./SourceData/train_original_cl.csv")
y_train_log_cl = train_log_cl$SalePrice
x_train_log_cl = subset (train_log_cl, select = -SalePrice)
y_train_log_cl = as.factor(y_train_log_cl)
y_test_log_cl = as.factor(y_test_log_cl)
summary(y_train_ori_cl)
```

```{r sqrtTestTrainCl, include=FALSE}
test_sqrt_cl = read.csv("./SourceData/test_original_cl.csv")
y_test_sqrt_cl = test_sqrt_cl$SalePrice
x_test_sqrt_cl = subset (test_sqrt_cl, select = -SalePrice)
train_sqrt_cl = read.csv("./SourceData/train_original_cl.csv")
y_train_sqrt_cl = train_sqrt_cl$SalePrice
x_train_sqrt_cl = subset (train_sqrt_cl, select = -SalePrice)
y_train_sqrt_cl = as.factor(y_train_sqrt_cl)
y_test_sqrt_cl = as.factor(y_test_sqrt_cl)
summary(y_train_ori_cl)
```
```
### 1. PCA

```{r PCA}
x_train_ori.pr = prcomp(x_train_ori)
# summary(X.pr)
```

```{r plotPCA}
screeplot(x_train_ori.pr,type="lines")
```

```{r}
# sum(0.1876,0.0948,0.06105,0.05679,0.04432,0.03818,0.03808,0.03684,0.03495,0.03262,0.03129,0.03082,0.0276,0.02717,0.02593,0.02456,0.02272)

plot(x_train_ori.pr$x[,1:17], col=2:3, xlab="Z1", ylab="Z2", pch=19)
```

```{r kernalPCA}
KPCA = kpca(x_train_ori, kernel="rbfdot", kpar=list(sigma=1))
# eig(KPCA)
plot(pcv(KPCA),col=2:3, xlab="Z1", ylab="Z2", pch=2)
```