library(dplyr)
library(purrr)
local({
hook_inline = knitr::knit_hooks$get('inline')
knitr::knit_hooks$set(inline = function(x) {
res = hook_inline(x)
if (is.numeric(x)) sprintf('$%s$', res) else res
})
})
# define printf function
printf <- function(...)print(sprintf(...))
# Import model libraries
library(pls)
library(randomForest)
library(gam)
library(glmnet)
library(ggplot2)
library(corrplot)
library(tidyverse)
library(caret)
library(mgcv)
library(Metrics)
library(visreg)
library(boot)
library('ggthemes')
library('scales')
library('mice')
library('data.table')
library('gridExtra')
library('GGally')
library('e1071')
library(Rmisc)
library(ggrepel)
library(psych)
library(xgboost)
HousePricing = read.csv("./SourceData/train_new.csv")
HousePricing = subset(HousePricing,select=-Id)
# The number of columns and rows
paste("Original training data set has",dim(HousePricing)[1], "rows and", dim(HousePricing)[2], "columns")
# # The percentage of data missing in train
# paste("The percentage of data missing in the original training data set is ", round(sum(is.na(HousePricing)) / (nrow(HousePricing) *ncol(HousePricing)),4)*100,"%",sep = "")
# The number of duplicated rows
paste("The number of duplicated rows are", nrow(HousePricing) - nrow(unique(HousePricing)))
paste("Number of Categorical Predictors:",sum(sapply(HousePricing[,1:84],typeof) == "character"))
paste("Number of Numeric Predictors:",dim(HousePricing)[2]-sum(sapply(HousePricing[,1:84],typeof) == "character")-1)
paste("Number of target label:", 1)
summary(HousePricing$SalePrice)
hist(HousePricing$SalePrice,col="blue",breaks = 25,main = "Distribution of Sale Price", xlab = "Sale Price")
numericVars <- which(sapply(HousePricing, is.numeric)) #index vector numeric variables
numericVarNames <- names(numericVars) #saving names vector for use later on
# cat('There are', length(numericVars), 'numeric variables')
all_numVar <- HousePricing[, numericVars]
cor_numVar <- cor(all_numVar, use="pairwise.complete.obs") #correlations of all numeric variables
#sort on decreasing correlations with SalePrice
cor_sorted <- as.matrix(sort(cor_numVar[,'SalePrice'], decreasing = TRUE))
#select only high corelations
CorHigh <- names(which(apply(cor_sorted, 1, function(x) abs(x)>0.5)))
cor_numVar <- cor_numVar[CorHigh, CorHigh]
corrplot.mixed(cor_numVar, tl.col="black", tl.pos = "lt")
ggplot(data=HousePricing[!is.na(HousePricing$SalePrice),], aes(x=factor(OverallQual), y=SalePrice))+
geom_boxplot(col='blue') + labs(x='Overall Quality') +
scale_y_continuous(breaks= seq(0, 800000, by=100000), labels = comma)
qplot(HousePricing$GrLivArea, HousePricing$SalePrice,col=HousePricing$GrLivArea>4500,xlab = "Ground Living Area (sqft)",ylab = "SalePrice")
pander(HousePricing[HousePricing$GrLivArea > 4500,][c("GrLivArea", "OverallQual","SalePrice")])
# remove ID column
HousePricing$Id = NULL
HousePricing = HousePricing[HousePricing$GrLivArea<4500,]
# for using later
numericVars <- which(sapply(HousePricing, is.numeric))
numericVarNames <- names(numericVars)
# LotFrontage
# compute the median of neighbor, na.rm means compute medians without NA
paste("There are",dim(HousePricing[is.na(HousePricing$LotFrontage),])[1],"rows with NAs in LotFrontage column")
neighbor_Median  = HousePricing %>%
select(LotFrontage, Neighborhood) %>%
group_by(Neighborhood) %>%
summarise(LotFrontage = median(LotFrontage, na.rm = T))
print(neighbor_Median)
# replace the LotFrontage NA with its neighbor's Lotfrontage's median.
for (i in 1:nrow(HousePricing))
{
if(is.na(HousePricing$LotFrontage[i])){
HousePricing$LotFrontage[i] <- as.integer(median(HousePricing$LotFrontage[HousePricing$Neighborhood==HousePricing$Neighborhood[i]], na.rm=TRUE))
}
}
# Alley, NA means no alley.
HousePricing$Alley[is.na(HousePricing$Alley)] = 'None'
HousePricing$Alley = as.factor(HousePricing$Alley)
# For utilites, there are two NAs, one row in one category, and the rest all share the same category
# Therefore we remove the entire column
# table(HousePricing$Utilities)
HousePricing$Utilities = NULL
# Pool variables are the ones with most NAs
# 1. Assign NAs to None (suppose those houses do not have a pool)
table(HousePricing$PoolQC)
HousePricing$PoolQC[is.na(HousePricing$PoolQC)] = "None"
# 2. Change it to Ordinal (scale them into numbers)
HousePricing$PoolQC=recode(HousePricing$PoolQC,'None' = 0,'Fa' = 1,'TA' = 2,'Gd' = 3,'Ex' = 4)
# Fence
HousePricing$Fence[is.na(HousePricing$Fence)] = "None"
HousePricing$Fence = as.factor(HousePricing$Fence)
# Miscellaneous features
HousePricing$MiscFeature[is.na(HousePricing$MiscFeature)] = "None"
HousePricing$MiscFeature = as.factor(HousePricing$MiscFeature)
# garage
# replace NAs with the year that the house was built
HousePricing$GarageYrBlt[is.na(HousePricing$GarageYrBlt)] <- HousePricing$YearBuilt[is.na(HousePricing$GarageYrBlt)]
# garage type dost not seem to be ordinal, then convert to factors
HousePricing$GarageType[is.na(HousePricing$GarageType)] = "None"
HousePricing$GarageType = as.factor(HousePricing$GarageType)
# convert to ordinals
HousePricing$GarageFinish[is.na(HousePricing$GarageFinish)] = "None"
HousePricing$GarageFinish=recode(HousePricing$GarageFinish,'None' = 0,'Unf' = 1,'RFn' = 2,'Fin' = 3)
HousePricing$GarageQual[is.na(HousePricing$GarageQual)] = "None"
HousePricing$GarageQual=recode(HousePricing$GarageQual,'None' = 0,'Po' = 1,'Fa' = 2,'TA' = 3,'Gd' = 4,'Ex' = 5)
HousePricing$GarageCond[is.na(HousePricing$GarageCond)] = "None"
HousePricing$GarageCond=recode(HousePricing$GarageCond,'None' = 0,'Po' = 1,'Fa' = 2,'TA' = 3,'Gd' = 4,'Ex' = 5)
HousePricing$FireplaceQu[is.na(HousePricing$FireplaceQu)] = "None"
HousePricing$FireplaceQu=recode(HousePricing$FireplaceQu,'None' = 0,'Po' = 1,'Fa' = 2,'TA' = 3,'Gd' = 4,'Ex' = 5)
#electric
# only one missing value, convert it to most common type
HousePricing$Electrical[is.na(HousePricing$Electrical)] = "SBrkr"
HousePricing$Electrical = as.factor(HousePricing$Electrical)
# basement
length(which(is.na(HousePricing$BsmtQual) & is.na(HousePricing$BsmtCond) & is.na(HousePricing$BsmtExposure) & is.na(HousePricing$BsmtFinType1) & is.na(HousePricing$BsmtFinType2)))
HousePricing[!is.na(HousePricing$BsmtCond) & (is.na(HousePricing$BsmtFinType1)|is.na(HousePricing$BsmtQual)|is.na(HousePricing$BsmtExposure)|is.na(HousePricing$BsmtFinType2)), c('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2')]
HousePricing$BsmtFinType2[333] = names(sort(table(HousePricing$BsmtFinType2),decreasing = TRUE))[1]
HousePricing$BsmtExposure[949] = names(sort(table(HousePricing$BsmtExposure),decreasing = TRUE))[1]
# convert to ordinal
HousePricing$BsmtExposure[is.na(HousePricing$BsmtExposure)] = 'None'
HousePricing$BsmtExposure=recode(HousePricing$BsmtExposure,'None' = 0,'No' = 1,'Mn' = 2,'Av' = 3,'Gd' = 4)
HousePricing$BsmtQual[is.na(HousePricing$BsmtQual)] = 'None'
HousePricing$BsmtQual=recode(HousePricing$BsmtQual,'None' = 0,'Po' = 1,'Fa' = 2,'TA' = 3,'Gd' = 4,'Ex' = 5)
HousePricing$BsmtCond[is.na(HousePricing$BsmtCond)] = 'None'
HousePricing$BsmtCond=recode(HousePricing$BsmtCond,'None' = 0,'Po' = 1,'Fa' = 2,'TA' = 3,'Gd' = 4,'Ex' = 5)
HousePricing$BsmtFinType1[is.na(HousePricing$BsmtFinType1)] = 'None'
HousePricing$BsmtFinType1=recode(HousePricing$BsmtFinType1,'None' = 0,'Unf' = 1,'LwQ' = 2,'Rec' = 3,'BLQ' = 4,'ALQ' = 5, 'GLQ' = 6)
HousePricing$BsmtFinType2[is.na(HousePricing$BsmtFinType2)] = 'None'
HousePricing$BsmtFinType2=recode(HousePricing$BsmtFinType2,'None' = 0,'Unf' = 1,'LwQ' = 2,'Rec' = 3,'BLQ' = 4,'ALQ' = 5, 'GLQ' = 6)
# Mas
# missing value set to none
HousePricing$MasVnrType[is.na(HousePricing$MasVnrType)] = 'None'
HousePricing$MasVnrType = as.factor(HousePricing$MasVnrType)
HousePricing$MasVnrArea[(is.na(HousePricing$MasVnrArea))] = 0
# MS Zoning
# categorical --> factor
HousePricing$MSZoning = as.factor(HousePricing$MSZoning)
# street
# categorical --> factor
HousePricing$Street = as.factor(HousePricing$Street)
HousePricing$LotShape=recode(HousePricing$LotShape,'IR3' = 0,'IR2' = 1,'IR1' = 2,'Reg' =2)
HousePricing$LotConfig = as.factor(HousePricing$LotConfig)
# House condition
HousePricing$Condition1 = as.factor(HousePricing$Condition1)
HousePricing$Condition2 = as.factor(HousePricing$Condition2)
# categorical
HousePricing$LandContour = as.factor(HousePricing$LandContour)
# categorical
HousePricing$RoofStyle = as.factor(HousePricing$RoofStyle)
# ordinal
HousePricing$LandSlope=recode(HousePricing$LandSlope,'Sev' = 0,'Mod' = 1,'Gtl' = 2)
# categorical
HousePricing$BldgType = as.factor(HousePricing$BldgType)
HousePricing$HouseStyle=as.factor(HousePricing$HouseStyle)
HousePricing$RoofMatl=as.factor(HousePricing$RoofMatl)
HousePricing$Exterior1st=as.factor(HousePricing$Exterior1st)
HousePricing$Exterior2nd=as.factor(HousePricing$Exterior2nd)
HousePricing$ExterQual=recode(HousePricing$ExterQual,'Po' = 0,'Fa' = 1,'TA' = 2,'Gd' = 3,'Ex' = 4)
HousePricing$ExterCond=recode(HousePricing$ExterCond,'Po' = 0,'Fa' = 1,'TA' = 2,'Gd' = 3,'Ex' = 4)
HousePricing$Foundation = as.factor(HousePricing$Foundation)
HousePricing$PavedDrive=recode(HousePricing$PavedDrive,'N' = 0,'P' = 1,'Y' = 2)
HousePricing$Heating = as.factor(HousePricing$Heating)
HousePricing$HeatingQC=recode(HousePricing$HeatingQC,'Po' = 0,'Fa' = 1,'TA' = 2,'Gd' = 3,'Ex' = 4)
HousePricing$CentralAir=recode(HousePricing$CentralAir,'N' = 0,'Y' = 1)
# Kitchen variables
HousePricing$KitchenQual=recode(HousePricing$KitchenQua,'Po' = 0,'Fa' = 1,'TA' = 2,'Gd' = 3,'Ex' = 4)
HousePricing$Functional=recode(HousePricing$Functional,'Sal' = 0,'Sev' = 1,'Maj2' = 2,'Maj1' = 3,'Mod' = 4,'Min2' = 5,'Min1' = 6,'Typ' = 7)
# Neighborhood
HousePricing$Neighborhood = as.factor(HousePricing$Neighborhood)
# Sale type
HousePricing$SaleType = as.factor(HousePricing$SaleType)
# Sale condition
HousePricing$SaleCondition = as.factor(HousePricing$SaleCondition)
# drop month sold
HousePricing$MoSold = NULL
HousePricing$MSSubClass = as.factor(HousePricing$MSSubClass)
# switch to factor
HousePricing$MSSubClass=recode(HousePricing$MSSubClass,'20' = '1-STORY 1946+',
'30' = '1-STORY 1945-','40' = '1-STORY Unf Attic',
'45' = "1/2 STORY Unf Attic",'50' = '1/2 STORY Fin',
'60' = '2-STORY+','70' = '2-STORY 1945-','80' = 'SPLIT OR MULTI-LEVEL',
'85' = 'SPLIT FOYER','90' = 'DUPLEX', '120' = '1-STORY PUD 1946+',
'150' = '1/2 STORY PUD','160' = '2-STORY PUD 1946+',
'180' = 'PUD - MULTILEVEL',' 190' = '2 FAMILY CONVERSION')
# draw a plot of correlation between numerical variables in original data
#which(sapply(HousePricing, is.numeric))
numericVars = which(sapply(HousePricing, is.numeric))#numericVars
factorVars = which(sapply(HousePricing, is.factor))
cat('There are', length(numericVars), 'numeric variables, and', length(factorVars), 'categoric variables')
numVar = HousePricing[,numericVars]
cor_numVar =(cor(numVar))
cor_sorted = as.matrix(sort(cor_numVar[,"SalePrice"],decreasing = TRUE))
#cor_sorted
# CorHigh <- names(which(apply(cor_sorted, 1, function(x) abs(x)>0.5)))
# cor_numVar <- cor_numVar[CorHigh, CorHigh]
# corrplot.mixed(cor_numVar, tl.col="black", tl.pos = "lt", tl.cex = 0.7,cl.cex = .7, number.cex=.7)
# draw a importance plot of all predictors in the original data
# set.seed(2018)
# quick_RF <- randomForest(x=HousePricing[,-78], y=HousePricing$SalePrice, ntree=100,importance=TRUE)
# imp_RF <- importance(quick_RF)
# imp_DF <- data.frame(Variables = row.names(imp_RF), MSE = imp_RF[,1])
# imp_DF <- imp_DF[order(imp_DF$MSE, decreasing = TRUE),]
# ggplot(imp_DF[1:15,], aes(x=reorder(Variables, MSE), y=MSE, fill=MSE)) + geom_bar(stat = 'identity') + labs(x = 'Variables', y= '% increase MSE if variable is randomly permuted') + coord_flip() + theme(legend.position="none")
n1 <- ggplot(HousePricing[!is.na(HousePricing$SalePrice),], aes(x=Neighborhood, y=SalePrice)) +
geom_bar(stat='summary', fun.y = "median", fill='blue') +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
scale_y_continuous(breaks= seq(0, 800000, by=50000), labels = comma) +
geom_label(stat = "count", aes(label = ..count.., y = ..count..), size=3) +
geom_hline(yintercept=163000, linetype="dashed", color = "red")
#dashed line is median SalePrice
n2 <- ggplot(data=HousePricing, aes(x=Neighborhood)) +
geom_histogram(stat='count')+
geom_label(stat = "count", aes(label = ..count.., y = ..count..), size=3)+
theme(axis.text.x = element_text(angle = 45, hjust = 1))
grid.arrange(n1, n2)
########################### further feature engineer ###################
# whether remod
HousePricing$Remod = ifelse(HousePricing$YearBuilt == HousePricing$YearRemodAdd,0,1)
# the age of house
HousePricing$Age = as.numeric(HousePricing$YrSold) - HousePricing$YearRemodAdd
# whether is new
HousePricing$isnew = ifelse(HousePricing$YrSold == HousePricing$YearBuilt,1,0)
# total area.
HousePricing$TotalSqFeet = HousePricing$GrLivArea+HousePricing$TotalBsmtSF
# count the totol number of bathroom in the hourse
HousePricing$TotBathrooms <- HousePricing$FullBath + (HousePricing$HalfBath*0.5) + HousePricing$BsmtFullBath + (HousePricing$BsmtHalfBath*0.5)
HousePricing$TotalPorchSF <- HousePricing$OpenPorchSF + HousePricing$EnclosedPorch + HousePricing$X3SsnPorch + HousePricing$ScreenPorch
# draw a correlation plot after combine some variable
numericVars = which(sapply(HousePricing, is.numeric))#numericVars
factorVars = which(sapply(HousePricing, is.factor))
numVar = HousePricing[,numericVars]
cor_numVar =(cor(numVar))
cor_sorted = as.matrix(sort(cor_numVar[,"SalePrice"],decreasing = TRUE))
#cor_sorted
CorHigh <- names(which(apply(cor_sorted, 1, function(x) abs(x)>0.5)))
cor_numVar <- cor_numVar[CorHigh, CorHigh]
corrplot.mixed(cor_numVar, tl.col="black", tl.pos = "lt", tl.cex = 0.7,cl.cex = .7, number.cex=.7)
# choose some true numerical variable to normalize(not include the encoded part)
numericVarNames <- numericVarNames[!(numericVarNames %in% c('MSSubClass', 'MoSold', 'YrSold', 'SalePrice', 'OverallQual', 'OverallCond'))]
numericVarNames <- append(numericVarNames, c('Age', 'TotalPorchSF', 'TotBathrooms', 'TotalSqFeet'))
# delete some highly correlated variables
HousePricing = subset(HousePricing, select = -c(GrLivArea,ExterQual,GarageArea,X1stFlrSF,
TotRmsAbvGrd,TotalBsmtSF,GarageYrBlt,FullBath,
HalfBath,YearRemodAdd,BsmtHalfBath,BsmtFullBath))
DFnumeric <- HousePricing[, names(HousePricing) %in% numericVarNames]
DFfactors <- HousePricing[, !(names(HousePricing) %in% numericVarNames)]
DFfactors <- DFfactors[, names(DFfactors) != 'SalePrice']
########################## Normalizing the numerical data #########################
predf = scale(DFnumeric,center = T,scale = T)
############ one-hot encoding and combine with scaled numerical data ###########
dfdummies = model.matrix(~.-1,DFfactors) %>% as.data.frame()
dim(dfdummies)
dim(predf)
newdata = cbind(predf,dfdummies)
newdata$SalePrice = HousePricing$SalePrice
set.seed(1)
vault = sample(1:nrow(newdata), nrow(newdata)*0.15)
dVault = newdata[vault,]
newdata = newdata[-vault,]
oriHouseP = newdata
oriHouseP$SalePrice = newdata$SalePrice
sumOri = summary(oriHouseP$SalePrice)
sumOri
logHouseP = newdata
logHouseP$SalePrice = log(newdata$SalePrice)
sumLog = summary(logHouseP$SalePrice)
hist(logHouseP$SalePrice, col="blue", main="Histogram of Logged Sale Price")
sumLog
sqrtHouseP = newdata
sqrtHouseP$SalePrice = '^'(newdata$SalePrice,1/4)
sumSqrt = summary(sqrtHouseP$SalePrice)
hist(sqrtHouseP$SalePrice, col="blue", main="Histogram of Forth Rooted Sale Price")
sumSqrt
bsF<- function(datadf, randomizer){
set.seed(randomizer)
sample = sample(dim(datadf)[1],dim(datadf)[1],replace = T)
btnewdata = datadf[sample,]
return(btnewdata)
}
newOriHouseP = bsF(oriHouseP, 1234)
newLogHouseP = bsF(logHouseP, 1111)
newSqrtHouseP = bsF(sqrtHouseP, 1332)
toCsv <- function(df, fileName){
set.seed(10)
randS = sample(1:nrow(df), nrow(df)*0.7)
train = df[randS,]
test = df[-randS,]
write.csv(train,paste("./SourceData/train_",fileName, ".csv",sep=""), row.names = FALSE)
write.csv(test,paste("./SourceData/test_",fileName, ".csv",sep=""), row.names = FALSE)
}
toCsv(oriHouseP, "original")
toCsv(logHouseP, "log")
toCsv(sqrtHouseP, "sqrt")
test_ori = read.csv("./SourceData/test_original.csv")
y_test_ori = test_ori$SalePrice
x_test_ori = subset (test_ori, select = -SalePrice)
train_ori = read.csv("./SourceData/train_original.csv")
y_train_ori = train_ori$SalePrice
x_train_ori = subset (train_ori, select = -SalePrice)
y_train_ori = as.numeric(y_train_ori)
y_test_ori = as.numeric(y_test_ori)
summary(y_train_ori)
test_log = read.csv("./SourceData/test_log.csv")
y_test_log = test_log$SalePrice
x_test_log = subset (test_log, select = -SalePrice)
train_log = read.csv("./SourceData/train_log.csv")
y_train_log = train_log$SalePrice
x_train_log = subset (train_log, select = -SalePrice)
y_train_log = as.numeric(y_train_log)
y_test_log = as.numeric(y_test_log)
summary(y_train_log)
test_sqrt = read.csv("./SourceData/test_sqrt.csv")
y_test_sqrt = test_sqrt$SalePrice
x_test_sqrt = subset (test_sqrt, select = -SalePrice)
train_sqrt = read.csv("./SourceData/train_sqrt.csv")
y_train_sqrt = train_sqrt$SalePrice
x_train_sqrt = subset (train_sqrt, select = -SalePrice)
y_train_sqrt = as.numeric(y_train_sqrt)
y_test_sqrt = as.numeric(y_test_sqrt)
summary(y_train_sqrt)
# ori
k = 5
lm_ori_accuracy = rep(0,k)
for (i in 1:k){
set.seed(100+i)
sample = sample(nrow(train_ori),nrow(train_ori),replace = T)
ori_train = train_ori[sample,]
train = sample(nrow(ori_train),0.7*nrow(ori_train))
training_dataset = ori_train[train,]
validation_dataset = ori_train[-train,]
lm_ori_model = glm(formula = SalePrice ~.,data = training_dataset)
lm_ori_pred = predict(lm_ori_model,newdata = validation_dataset)
lm_ori_accuracy[i] = mean(((validation_dataset$SalePrice - lm_ori_pred)/validation_dataset$SalePrice<=0.2) & ((validation_dataset$SalePrice - lm_ori_pred)/validation_dataset$SalePrice>=-0.05))
}
lm_ori_aver_accuracy = mean(lm_ori_accuracy)
k = 5
lm_log_accuracy = rep(0,k)
for (i in 1:k){
set.seed(111+i)
sample = sample(nrow(train_log),nrow(train_log),replace = T)
log_train = train_log[sample,]
train = sample(nrow(log_train),0.7*nrow(log_train))
training_dataset = log_train[train,]
validation_dataset = log_train[-train,]
lm_log_model = glm(formula = SalePrice ~.,data = training_dataset)
lm_log_pred = predict(lm_log_model,newdata = validation_dataset)
lm_log_pred = exp(lm_log_pred)
lm_log_accuracy[i] = mean(((exp(validation_dataset$SalePrice)-(lm_log_pred))/exp(validation_dataset$SalePrice)<=0.2) & ((exp(validation_dataset$SalePrice)-(lm_log_pred))/exp(validation_dataset$SalePrice)>=-0.05))
}
lm_log_aver_accuracy = mean(lm_log_accuracy)
lm_sqrt_accuracy = rep(0,k)
for (i in 1:k){
set.seed(150+i)
sample = sample(nrow(train_sqrt),nrow(train_sqrt),replace = T)
sqrt_train = train_sqrt[sample,]
train = sample(nrow(sqrt_train),0.7*nrow(sqrt_train))
training_dataset = sqrt_train[train,]
validation_dataset = sqrt_train[-train,]
lm_sqrt_model = glm(formula = SalePrice ~.,data = training_dataset)
lm_sqrt_pred = predict(lm_sqrt_model,newdata = validation_dataset)
lm_sqrt_pred = lm_sqrt_pred^4
lm_sqrt_accuracy[i] = mean((((validation_dataset$SalePrice)^4- (lm_sqrt_pred))/(validation_dataset$SalePrice)^4<=0.2) & ((validation_dataset$SalePrice)^4- (lm_sqrt_pred))/(validation_dataset$SalePrice)^4>=-0.05)
}
lm_sqrt_aver_accuracy = mean(lm_sqrt_accuracy)
# Multiple R-squared: 0.9475,
# Adjusted R-squared: 0.9345
# F-statistic:  73.09  on 289 and 1170 DF,
# p-value: < 2.2e-16
#accuracy for original
lm_ori_accuracy
lm_ori_aver_accuracy
printf("We have the accuracy of the linear model approximately %.2f%%", lm_ori_aver_accuracy*100)
# accuracy for log
lm_log_accuracy
lm_log_aver_accuary = mean(lm_log_accuracy)
printf("We have the accuracy of the linear model after log transformation approximately %.2f%%", lm_log_aver_accuary*100)
## accuracy for sqrt
lm_sqrt_accuracy
lm_sqrt_aver_accuracy = mean(lm_sqrt_accuracy)
printf("We have the accuracy of the linear model after sqrt transformation approximately %.2f%%", lm_sqrt_aver_accuracy*100)
lm_accuracy_df = data.frame(lm = c('accuracy'),ori_accuracy = c(lm_ori_aver_accuracy),
log_accuracy = c(lm_log_aver_accuary),
sqrt_accuracy = c(lm_sqrt_aver_accuracy))
#lm_pred  = exp(lm_pred)
#result_lm_model = data.frame(Id = testing_data$Id, SalePrice = lm_pred)
# Need to figure out how many independent variables then set mtry
# set mtry to be square root of total number of independent variables
totalIV = length(colnames(train_ori))
rfTrain_ori=randomForest(SalePrice~.,data=train_ori, mtry=sqrt(totalIV),importance =TRUE)
pander(rfTrain_ori)
# Need to figure out how many independent variables then set mtry
# set mtry to be square root of total number of independent variables
totalIV = length(colnames(train_log))
rfTrain_log=randomForest(SalePrice~.,data=train_log, mtry=sqrt(totalIV),importance =TRUE)
pander(rfTrain_log)
# Need to figure out how many independent variables then set mtry
# set mtry to be square root of total number of independent variables
totalIV = length(colnames(train_sqrt))
rfTrain_sqrt=randomForest(SalePrice~.,data=train_sqrt, mtry=sqrt(totalIV),importance =TRUE)
pander(rfTrain_sqrt)
rfYhat_ori = predict(rfTrain_ori, newdata=x_test_ori)
#table(y_test, rfYhat)
rf_accuracy_ori = mean(((y_test_ori - rfYhat_ori)/y_test_ori<=0.2) & ((y_test_ori - rfYhat_ori)/y_test_ori>=-0.05))
printf("We have the accuracy of the model approximately %.2f%%", rf_accuracy_ori*100)
rfYhat_log = predict(rfTrain_log, newdata=x_test_log)
#table(y_test, rfYhat)
rf_accuracy_log = mean(((exp(y_test_log) - exp(rfYhat_log))/exp(y_test_log)<=0.2) & ((exp(y_test_log) - exp(rfYhat_log))/exp(y_test_log)>=-0.05))
printf("We have the accuracy of the model approximately %.2f%%", rf_accuracy_log*100)
rfYhat_sqrt = predict(rfTrain_sqrt, newdata=x_test_sqrt)
#table(y_test, rfYhat)
rf_accuracy_sqrt = mean(((y_test_sqrt^4 - rfYhat_sqrt^4)/y_test_sqrt^4<=0.2) & ((y_test_sqrt^4 - rfYhat_sqrt^4)/y_test_sqrt^4>=-0.05))
printf("We have the accuracy of the model approximately %.2f%%", rf_accuracy_sqrt*100)
# another plot
importance = importance(rfTrain_ori)
importancedf =  data.frame(Variables = row.names(importance), MSE = importance[,1])
importancedf <- importancedf[order(importancedf$MSE, decreasing = TRUE),]
ggplot(importancedf[1:10,], aes(x=reorder(Variables, MSE), y=MSE, fill=MSE)) + geom_bar(stat = 'identity') + labs(x = 'Variables', y= 'Importance') + coord_flip() + theme(legend.position="none")
set.seed(123)
# computing model performance metrics
# pander(data.frame( R2 = R2(rfYhat, y_test_ori),
#             RMSE = RMSE(rfYhat, y_test_ori),
#             MAE = MAE(rfYhat, y_test_ori)), title="Cross Validation for Random Forest")
r2rf = c(R2(rfYhat_ori, y_test_ori),R2(exp(rfYhat_log), exp(y_test_log)),R2(rfYhat_sqrt^4, y_test_sqrt^4))
rmserf = c(RMSE(rfYhat_ori, y_test_ori),RMSE(exp(rfYhat_log), exp(y_test_log)),RMSE(rfYhat_sqrt^4, y_test_sqrt^4))
accrf = c(rf_accuracy_ori,rf_accuracy_log,rf_accuracy_sqrt)
x_train_ori.pr = prcomp(x_train_ori)
#print(summary(x_train_ori.pr))
plot(c(1:32),x_train_ori.pr$sdev[1:32],xlab="Principal Components",type ="l",ylab="Prop. Variance Explained",main="Prop. Variance Elbow")
# New data list cleansed by PCA size of 868*32, which can take the place of original data.
# Each column is one PC, PC's importance decreases by column number.
# Try CV on number of PC you drop from right to left and know how many PCs are best to use.
newdata = x_train_ori.pr$x[,1:32]
printf("Rows num of training set: %i",nrow(x_train_ori))
printf("Columns num of training set: %i",ncol(x_train_ori))
printf("Rows num of training set after PCA: %i",nrow(newdata))
printf("Columns num of training set after PCA: %i",ncol(newdata))
set.seed(2)
# Need help with PCA analysis
# pcr.fit=pcr(SalePrice~., data=HousePricing, scale=TRUE, validation ="CV")
# setwd("~/Documents/DS502/DS502FinalProject")
#train.original = read.csv("./SourceData/train_original.csv")
# PCA portion
train = sample(dim(train_ori)[1],size = dim(train_ori)[1]*0.7)
train.subset = train_ori[train,]
test = train_ori[-train,]
# bootstrap again
train.bs = sample(dim(train.subset)[1],replace = T)
train.df = train.subset[train.bs,]
# define a function to generate scree plots
pcaCharts <- function(x) {
x.var <- x$sdev ^ 2
x.pvar <- x.var/sum(x.var)
# print("proportions of variance:")
# print(x.pvar)
par(mfrow=c(2,2))
plot(x.pvar,xlab="Principal component", ylab="Proportion of variance explained", ylim=c(0,1), type='b',cex.lab=1, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
plot(cumsum(x.pvar),xlab="Principal component", ylab="Cumulative Proportion of variance explained", ylim=c(0,1), type='b',cex.lab=0.9, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
screeplot(x,cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.8)
screeplot(x,type="l",cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.8)
par(mfrow=c(1,1))
}
x = subset(train.df, select = -c(SalePrice) )
train.pca <- prcomp(x ,center = TRUE)
pcaCharts(train.pca)
########################### insert an image  (pca_plot.png) #################################
pcr_fit = pcr(SalePrice~., data = train.df,validation = "CV")
# summary(pcr_fit)
validationplot(pcr_fit, val.type = "MSEP",cex.lab=1, cex.axis=0.8, cex.main=1.5, cex.sub=0.8)
axis(side = 1, at = c(32), cex.axis=0.8)
abline(v = 32, col = "blue", lty = 5)
########################### insert another image  (pcr_mse.png) #################################
set.seed(2)
# PCA portion
train = sample(dim(train_ori)[1],size = dim(train_ori)[1]*0.7)
train.subset = train_ori[train,]
test = train_ori[-train,]
# bootstrap again
train.bs = sample(dim(train.subset)[1],replace = T)
train.df = train.subset[train.bs,]
# define a function to generate scree plots
pcaCharts <- function(x) {
x.var <- x$sdev ^ 2
x.pvar <- x.var/sum(x.var)
# print("proportions of variance:")
# print(x.pvar)
par(mfrow=c(2,2))
plot(x.pvar,xlab="Principal component", ylab="Proportion of variance explained", ylim=c(0,1), type='b',cex.lab=1, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
plot(cumsum(x.pvar),xlab="Principal component", ylab="Cumulative Proportion of variance explained", ylim=c(0,1), type='b',cex.lab=0.9, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
screeplot(x,cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.8)
screeplot(x,type="l",cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.8)
par(mfrow=c(1,1))
}
x = subset(train.df, select = -c(SalePrice) )
train.pca <- prcomp(x ,center = TRUE)
pcaCharts(train.pca)
########################### insert an image  (pca_plot.png) #################################
pcr_fit = pcr(SalePrice~., data = train.df,validation = "CV")
# summary(pcr_fit)
validationplot(pcr_fit, val.type = "MSEP",cex.lab=1, cex.axis=0.8, cex.main=1.5, cex.sub=0.8)
axis(side = 1, at = c(32), cex.axis=0.8)
abline(v = 32, col = "blue", lty = 5)
########################### insert another image  (pcr_mse.png) #################################
