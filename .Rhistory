x_train_cl = subset (train, select = -SalePrice)
y_train_cl = as.factor(y_train_cl)
y_test_cl = as.factor(y_test_cl)
summary(y_train_cl)
# Need to figure out how many independent variables then set mtry
# set mtry to be square root of total number of independent variables
totalIV = length(colnames(train))
rfTrain=randomForest(SalePrice~.,data=train, mtry=sqrt(totalIV)+1000,importance =TRUE)
pander(rfTrain)
rfYhat = predict(rfTrain, newdata=x_test)
#table(y_test, rfYhat)
accuracy = mean(abs(y_test - rfYhat)/y_test<=0.05)
printf("We have the accuracy of the model approximately %.2f%%", accuracy*100)
rfYhat = predict(rfTrain, newdata=x_test)
#table(y_test, rfYhat)
head(rfYhat)
head(y_test)
accuracy = mean(abs(y_test - rfYhat)/y_test<=0.05)
printf("We have the accuracy of the model approximately %.2f%%", accuracy*100)
numvar = which(sapply(combined_data, is.numeric))
vault = read.csv("./SourceData/test.csv")
vault = subset(vault, select=-Id)
HousePricing = read.csv("./SourceData/train.csv")
HousePricing = subset(HousePricing,select=-Id)
combined_data = HousePricing
#colnames(combined_data)
#glue("Dimension of Combined Dataset : {dim(combined_data)}")
#glimpse(combined_data)
# check number of NAs in each column:
cbind(colSums(is.na(combined_data)))
# drop ID column from the dataset
combined_data$Id = NULL
str(combined_data)
cbind(colSums(is.na(combined_data)))
# percentage of missing Values in dataset ####
cbind(colSums(is.na(combined_data)),colSums(is.na(combined_data))/(nrow(combined_data)) * 100)
# fixing Missing Values :
# MSZoning
# 4 missing values
unique(combined_data$MSZoning)
table(combined_data$MSZoning)
# Most of the values are populated with RL
combined_data$MSZoning[is.na(combined_data$MSZoning)] = "RL"
# LotFrontage : 486 missing values
#glue("Mean of LotFrontage : {mean(combined_data$LotFrontage, na.rm = T)} and Median of LotFrontage : {median(combined_data$LotFrontage, na.rm = T)}")
neighbor_Median  = combined_data %>%
select(LotFrontage, Neighborhood) %>%
group_by(Neighborhood) %>%
summarise(LotFrontage = median(LotFrontage, na.rm = T))
# alternative :
# neighbor_Median = aggregate(LotFrontage ~ Neighborhood, data = combined_data, median)
for (i in 1:nrow(combined_data))
{
if(is.na(combined_data$LotFrontage[i])){
temp = combined_data$Neighborhood[i]
combined_data$LotFrontage[i] = neighbor_Median$LotFrontage[neighbor_Median$Neighborhood == temp]
}
}
# Alley has 93.21% missing data
# and as per data description, NA =  no alley access
# Create a third alley type but before that convert it to character
combined_data$Alley = as.character(combined_data$Alley)
combined_data$Alley[is.na(combined_data$Alley)] = "None"
combined_data$Alley = as.factor(combined_data$Alley)
# utilites :  type of utilities available
combined_data$Utilities = as.factor(combined_data$Utilities)
unique(combined_data$Utilities)
table(combined_data$Utilities)
# One NA  and All other has AllPubs
combined_data$Utilities[is.na(combined_data$Utilities)] = "AllPub"
# But single value: no vaiation
#  Discard it
combined_data$Utilities = NULL
unique(combined_data$Exterior1st)
table(combined_data$Exterior1st)
combined_data$SalePrice[is.na(combined_data$Exterior1st)]
which.max(table(combined_data$Exterior1st))
# its in testing data
combined_data$Exterior1st[is.na(combined_data$Exterior1st)] = "VinylSd"
unique(combined_data$Exterior2nd)
table(combined_data$Exterior2nd)
which.max(table(combined_data$Exterior2nd))
combined_data$Exterior2nd[is.na(combined_data$Exterior2nd)] = "VinylSd"
# MasVbeType
unique(combined_data$MasVnrType)
table(combined_data$MasVnrType)
combined_data$MasVnrType[is.na(combined_data$MasVnrType)] = "None"
# Mass Veneer Area : 23 NAs
unique(combined_data$MasVnrArea)
combined_data$MasVnrArea[is.na(combined_data$MasVnrArea)] = median(combined_data$MasVnrArea, na.rm = T)
# Basement Details ####
# Basement Criteria : None for NA
unique(combined_data$BsmtQual)
combined_data$BsmtQual = as.character(combined_data$BsmtQual)
combined_data$BsmtQual[is.na(combined_data$BsmtQual)] = "None"
unique(combined_data$BsmtCond)
combined_data$BsmtCond = as.character(combined_data$BsmtCond)
combined_data$BsmtCond[is.na(combined_data$BsmtCond)] = "None"
combined_data$BsmtExposure = as.character(combined_data$BsmtExposure)
combined_data$BsmtExposure[is.na(combined_data$BsmtExposure)] = "None"
combined_data$BsmtFinType1 = as.character(combined_data$BsmtFinType1)
combined_data$BsmtFinType1[is.na(combined_data$BsmtFinType1)] = "None"
combined_data$BsmtFinType2 = as.character(combined_data$BsmtFinType2)
combined_data$BsmtFinType2[is.na(combined_data$BsmtFinType2)] = "None"
combined_data$BsmtFinSF1[is.na(combined_data$BsmtFinSF1)] = median(combined_data$BsmtFinSF1, na.rm = T)
cbind(table(combined_data$BsmtFinSF2))
# most 0s
combined_data$BsmtFinSF2[is.na(combined_data$BsmtFinSF2)] = 0
unique(combined_data$BsmtUnfSF)
combined_data$BsmtUnfSF[is.na(combined_data$BsmtUnfSF)] = median(combined_data$BsmtUnfSF, na.rm = T)
unique(combined_data$TotalBsmtSF)
combined_data$TotalBsmtSF[is.na(combined_data$TotalBsmtSF)] = median(combined_data$TotalBsmtSF, na.rm = T)
unique(combined_data$BsmtFullBath)
table(combined_data$BsmtFullBath)
combined_data$BsmtFullBath[is.na(combined_data$BsmtFullBath)] = 0
unique(combined_data$BsmtHalfBath)
table(combined_data$BsmtHalfBath)
combined_data$BsmtHalfBath[is.na(combined_data$BsmtHalfBath)] = 0
# Kitchen ####
unique(combined_data$KitchenQual)
table(combined_data$KitchenQual)
combined_data$KitchenQual[is.na(combined_data$KitchenQual)] = "TA"
## Electrical ####
unique(combined_data$Electrical)
table(combined_data$Electrical)
combined_data$Electrical[is.na(combined_data$Electrical)] = "SBrkr"
# functional
unique(combined_data$Functional)
table(combined_data$Functional)
combined_data$Functional[is.na(combined_data$Functional)] = "Typ"
unique(combined_data$FireplaceQu)
table(combined_data$FireplaceQu)
combined_data$FireplaceQu[is.na(combined_data$FireplaceQu)] = "None"
# Garage ####
unique(combined_data$GarageType)
table(combined_data$GarageType)
combined_data$GarageType[is.na(combined_data$GarageType)] = "None"
combined_data$GarageYrBlt[is.na(combined_data$GarageYrBlt)] = 0
table(combined_data$GarageFinish)
combined_data$GarageFinish[is.na(combined_data$GarageFinish)] = "None"
combined_data$GarageArea[is.na(combined_data$GarageArea)] = 0
combined_data$GarageCars[is.na(combined_data$GarageCars)] = 0
combined_data$GarageQual[is.na(combined_data$GarageQual)] = "None"
combined_data$GarageCond[is.na(combined_data$GarageCond)] = "None"
# Misclenneous #####
combined_data$PoolQC[is.na(combined_data$PoolQC)] = "None"
combined_data$Fence[is.na(combined_data$Fence)] = "None"
combined_data$MiscFeature[is.na(combined_data$MiscFeature)] = "None"
combined_data$MiscVal
# sales type ####
unique(combined_data$SaleType)
table(combined_data$SaleType)
combined_data$SaleType[is.na(combined_data$SaleType)] = 'WD'
# Changing characters to Factors variables
# apply(combined_data,1, function(x){ if(class(x) == "character"){x = as.factor(x)}})
combined_data = as.data.frame(unclass(combined_data))
# some numeric data to factor
combined_data$MSSubClass = as.factor(combined_data$MSSubClass)
numvar = which(sapply(combined_data, is.numeric))
catvar = which(sapply(combined_data, is.factor))
numdata = combined_data[,numvar]
numcor =(cor(numdata))
corsorted = as.matrix(sort(numcor[,"SalePrice"],decreasing = TRUE))
CorHigh <- names(which(apply(corsorted, 1, function(x) abs(x)>0.5)))
numcor <- numcor[CorHigh, CorHigh]
corrplot.mixed(numcor, tl.col="black", tl.pos = "lt", tl.cex = 0.7,cl.cex = .7, number.cex=.7)
temp = subset(combined_data,select=-SalePrice)
# standardize numerical data
numeric = select_if(temp,is.numeric)
stnumer = scale(numeric,center = T,scale = T)
convFact = select_if(temp,is.factor)
# one hot
convFact = model.matrix(~.-1,convFact) %>% data.frame()
SalePrice = log(combined_data$SalePrice)
# put standardized numerical data and categorical data in one data?
newdata = cbind(stnumer,convFact,SalePrice)
set.seed(1234)
sample = sample(dim(newdata)[1],dim(newdata)[1],replace = T)
btnewdata = newdata[sample,]
set.seed(1)
randS = sample(1:nrow(btnewdata), nrow(btnewdata)*0.7)
train = btnewdata[randS,]
test = btnewdata[-randS,]
write.csv(trainCl,"./SourceData/train_p3.csv", row.names = FALSE)
write.csv(train,"./SourceData/train_p3.csv", row.names = FALSE)
write.csv(test,"./SourceData/test_p3.csv", row.names = FALSE)
test = read.csv("./SourceData/test_p2.csv")
y_test = test$SalePrice
x_test = subset (test, select = -SalePrice)
train = read.csv("./SourceData/train_p2.csv")
y_train = train$SalePrice
x_train = subset (train, select = -SalePrice)
y_train = as.numeric(y_train)
y_test = as.numeric(y_test)
summary(y_train)
test = read.csv("./SourceData/test_p3.csv")
y_test = test$SalePrice
x_test = subset (test, select = -SalePrice)
train = read.csv("./SourceData/train_p3.csv")
y_train = train$SalePrice
x_train = subset (train, select = -SalePrice)
y_train = as.numeric(y_train)
y_test = as.numeric(y_test)
summary(y_train)
test_cl = read.csv("./SourceData/test_p_cl.csv")
y_test_cl = test$SalePrice
x_test_cl = subset (test, select = -SalePrice)
train_cl = read.csv("./SourceData/train_p_cl.csv")
y_train_cl = train_cl$SalePrice
x_train_cl = subset (train, select = -SalePrice)
y_train_cl = as.factor(y_train_cl)
y_test_cl = as.factor(y_test_cl)
summary(y_train_cl)
# Need to figure out how many independent variables then set mtry
# set mtry to be square root of total number of independent variables
totalIV = length(colnames(train))
rfTrain=randomForest(SalePrice~.,data=train, mtry=sqrt(totalIV)+1000,importance =TRUE)
pander(rfTrain)
rfYhat = predict(rfTrain, newdata=x_test)
#table(y_test, rfYhat)
head(rfYhat)
head(y_test)
accuracy = mean(abs(y_test - rfYhat)/y_test<=0.05)
printf("We have the accuracy of the model approximately %.2f%%", accuracy*100)
rfYhat = predict(rfTrain, newdata=x_test)
#table(y_test, rfYhat)
accuracy = mean(abs(y_test - rfYhat)/y_test<=0.05)
printf("We have the accuracy of the model approximately %.2f%%", accuracy*100)
# Need to figure out how many independent variables then set mtry
# set mtry to be square root of total number of independent variables
totalIV = length(colnames(train))
rfTrain=randomForest(SalePrice~.,data=train, mtry=sqrt(totalIV),importance =TRUE)
pander(rfTrain)
rfYhat = predict(rfTrain, newdata=x_test)
#table(y_test, rfYhat)
accuracy = mean(abs(y_test - rfYhat)/y_test<=0.05)
printf("We have the accuracy of the model approximately %.2f%%", accuracy*100)
rfYhat = predict(rfTrain, newdata=x_test)
#table(y_test, rfYhat)
accuracy = mean(abs(y_test - rfYhat)/y_test<=0.05)
printf("We have the accuracy of the model approximately %.2f%%", accuracy*100)
printf("We have the MSE of the model approximately %.1f", mean((rfYhat-test$SalePrice)^2))
importance = importance(rfTrain)
varImportance = head(data.frame(Variables = row.names(importance),
Importance =round(importance[, "%IncMSE"],1)),10)
rankImportance=varImportance%>%mutate(Rank=paste('#',dense_rank(desc(Importance))))
ggplot(rankImportance,aes(x=reorder(Variables,Importance),
y=Importance,fill=Importance))+
geom_bar(stat='identity') +
geom_text(aes(x = Variables, y = 0.5, label = Rank),
hjust=0, vjust=0.55, size = 4, colour = 'white') +
labs(x = 'Variables') +
coord_flip() +
theme_classic()
printf("We have the MSE of the model approximately %.5f", mean((rfYhat-test$SalePrice)^2))
importance = importance(rfTrain)
varImportance = head(data.frame(Variables = row.names(importance),
Importance =round(importance[, "%IncMSE"],1)),10)
rankImportance=varImportance%>%mutate(Rank=paste('#',dense_rank(desc(Importance))))
ggplot(rankImportance,aes(x=reorder(Variables,Importance),
y=Importance,fill=Importance))+
geom_bar(stat='identity') +
geom_text(aes(x = Variables, y = 0.5, label = Rank),
hjust=0, vjust=0.55, size = 4, colour = 'white') +
labs(x = 'Variables') +
coord_flip() +
theme_classic()
set.seed(123)
# computing model performance metrics
pander(data.frame( R2 = R2(rfYhat, y_test),
RMSE = RMSE(rfYhat, y_test),
MAE = MAE(rfYhat, y_test)), title="Cross Validation for Random Forest")
fit1 = gam(SalePrice ~ s(LotFrontage) + ns(YearRemodAdd,2) + MasVnrArea, data = train)
printf("Deviance of Model 1 approximately %.2f", deviance(fit1))
pred1 = predict(fit1, newdata=x_test)
par(mfrow=c(2,1))
accuracy = mean(abs(y_test - pred1)/y_test<=0.05)
printf("Accuracy of Model 1 approximately %.2f%%", accuracy*100)
plot(fit1 , se=TRUE , col="red")
fit2 = gam(SalePrice ~ LotFrontage + YearRemodAdd + s(MasVnrArea), data = train)
printf("Deviance of Model 2 approximately %.2f", deviance(fit2))
pred2 = predict(fit2, newdata=x_test)
accuracy = mean(abs(y_test - pred2)/y_test<=0.05)
printf("Accuracy of Model 2 approximately %.2f%%", accuracy*100)
plot(fit2 , se=TRUE , col="red")
fit3 = gam(SalePrice ~ ns(LotFrontage,3) + YearRemodAdd + s(MasVnrArea), data = train)
printf("Deviance of Model 3 approximately %.2f", deviance(fit3))
pred3 = predict(fit3, newdata=x_test)
accuracy = mean(abs(y_test - pred3)/y_test<=0.05)
printf("Accuracy of Model 3 approximately %.2f%%", accuracy*100)
plot(fit3 , se=TRUE , col="red")
anova(fit1,fit2,fit3,test="F")
set.seed(123)
pander(data.frame( R2 = R2(pred2, y_test),
RMSE = RMSE(pred2, y_test),
MAE = MAE(pred2, y_test)), title="Cross Validation of Model 2")
set.seed(2)
bs = sample(dim(train)[1],dim(train)[1],replace = T)
train = train[bs,]
train$SalePrice = log(train$SalePrice)
test$SalePrice = log(test$SalePrice)
X_train = model.matrix(SalePrice~.,data = train)[,-1]
X_test = model.matrix(SalePrice~.,test)[,-1]
y_train = train$SalePrice
y_test = test$SalePrice
set.seed(1234)
Ridge.Alpha=0
Ridge.Fit = glmnet(X_train, y_train, alpha=Ridge.Alpha, lambda=grid)
Ridge.Fitcv = cv.glmnet(X_train, y_train, alpha = Ridge.Alpha,nfolds = 10,type.measure = 'deviance')
best.lambda1 = Ridge.Fitcv$lambda.min
plot(Ridge.Fitcv)
Ridge.Pred <- predict(Ridge.Fit, s= best.lambda1, newx = X_test)
Ridge.Pred = exp(Ridge.Pred)
accuary = mean(abs(exp(y_test) - (Ridge.Pred))/exp(y_test) <=0.05)
printf("Accuracy of Ridge is approximately %.2f%%", accuracy*100)
sqrt(mean((Ridge.Pred -exp(y_test))^2))
pander(data.frame(R2 = R2(Ridge.Pred,y_test),RSME = RSME(Ridge.Pred,y_test),MAE = MAE(Ridge.Pred,y_test)),title="Cross Validation of Ridge Regression")
sqrt(mean((Ridge.Pred -exp(y_test))^2))
pander(data.frame(R2 = R2(Ridge.Pred,y_test),RMSE = RMSE(Ridge.Pred,y_test),MAE = MAE(Ridge.Pred,y_test)),title="Cross Validation of Ridge Regression")
Lasso.Alpha=1
grid=10^seq(10,-2, length =100)
set.seed(123)
Lasso.Fit = glmnet(X_train, y_train, alpha=Lasso.Alpha, lambda=grid)
Lasso.Fitcv = cv.glmnet(X_train, y_train, alpha = Lasso.Alpha,nfolds = 10,type.measure = 'deviance')
best.lambda = Lasso.Fitcv$lambda.min
best.lambda
plot(Lasso.Fitcv)
lasso.coef = predict(Lasso.Fit, s = best.lambda, type="coefficients")[1:53,]
sort(lasso.coef[lasso.coef!=0],decreasing = TRUE)
lasso.coef = predict(Lasso.Fit, s = best.lambda, type="coefficients")[1:53,]
sort(lasso.coef[lasso.coef!=0],decreasing = TRUE)
Lasso.Pred =  predict(Lasso.Fit, s= best.lambda, newx = X_test)
Lasso.Pred = exp(Lasso.Pred)
accuary1 = mean(abs(exp(y_test) - Lasso.Pred)/exp(y_test) <=0.05)
printf("Accuracy of Lasso is approximately %.2f%%", accuary1*100)
sqrt(mean((Lasso.Pred -exp(y_test))^2))
pander(data.frame(R2 = R2(Lasso.Pred,y_test), RMSE = RMSE(Lasso.Pred,y_test),MAE = MAE(Lasso.Pred,y_test)), title="Cross Validation of Lasso Regression")
Lasso.Pred =  predict(Lasso.Fit, s= best.lambda, newx = X_test)
Lasso.Pred = exp(Lasso.Pred)
accuary1 = mean(abs(exp(y_test) - Lasso.Pred)/exp(y_test) <=0.005)
printf("Accuracy of Lasso is approximately %.2f%%", accuary1*100)
Lasso.Pred =  predict(Lasso.Fit, s= best.lambda, newx = X_test)
Lasso.Pred = exp(Lasso.Pred)
accuary1 = mean(abs(exp(y_test) - Lasso.Pred)/exp(y_test) <=0.01)
printf("Accuracy of Lasso is approximately %.2f%%", accuary1*100)
Lasso.Pred =  predict(Lasso.Fit, s= best.lambda, newx = X_test)
Lasso.Pred = exp(Lasso.Pred)
accuary1 = mean(abs(exp(y_test) - Lasso.Pred)/exp(y_test) <=0.03)
printf("Accuracy of Lasso is approximately %.2f%%", accuary1*100)
Lasso.Pred =  predict(Lasso.Fit, s= best.lambda, newx = X_test)
Lasso.Pred = exp(Lasso.Pred)
accuary1 = mean(abs(exp(y_test) - Lasso.Pred)/exp(y_test) <=0.05)
printf("Accuracy of Lasso is approximately %.2f%%", accuary1*100)
lm_model = lm(formula = SalePrice ~.,data = train)
summary(lm_model)
# Multiple R-squared: 0.9475,
# Adjusted R-squared: 0.9345
# F-statistic:  73.09  on 289 and 1170 DF,
# p-value: < 2.2e-16
lm_pred = predict(lm_model, newdata = as.data.frame(test))
#lm_pred  = exp(lm_pred)
lm_pred
#result_lm_model = data.frame(Id = testing_data$Id, SalePrice = lm_pred)
# Multiple R-squared: 0.9475,
# Adjusted R-squared: 0.9345
# F-statistic:  73.09  on 289 and 1170 DF,
# p-value: < 2.2e-16
lm_pred = predict(lm_model, newdata = as.data.frame(test))
#lm_pred  = exp(lm_pred)
#result_lm_model = data.frame(Id = testing_data$Id, SalePrice = lm_pred)
set.seed(1234)
Ridge.Alpha=0
Ridge.Fit = glmnet(X_train, y_train, alpha=Ridge.Alpha, lambda=grid)
Ridge.Fitcv = cv.glmnet(X_train, y_train, alpha = Ridge.Alpha,nfolds = 10,type.measure = 'deviance')
test = read.csv("./SourceData/test_p3.csv")
y_test = test$SalePrice
x_test = subset (test, select = -SalePrice)
train = read.csv("./SourceData/train_p3.csv")
y_train = train$SalePrice
x_train = subset (train, select = -SalePrice)
y_train = as.numeric(y_train)
y_test = as.numeric(y_test)
summary(y_train)
test_cl = read.csv("./SourceData/test_p_cl.csv")
y_test_cl = test$SalePrice
x_test_cl = subset (test, select = -SalePrice)
train_cl = read.csv("./SourceData/train_p_cl.csv")
y_train_cl = train_cl$SalePrice
x_train_cl = subset (train, select = -SalePrice)
y_train_cl = as.factor(y_train_cl)
y_test_cl = as.factor(y_test_cl)
summary(y_train_cl)
set.seed(2)
bs = sample(dim(train)[1],dim(train)[1],replace = T)
train = train[bs,]
train$SalePrice = log(train$SalePrice)
test$SalePrice = log(test$SalePrice)
X_train = model.matrix(SalePrice~.,data = train)[,-1]
X_test = model.matrix(SalePrice~.,test)[,-1]
y_train = train$SalePrice
y_test = test$SalePrice
set.seed(1234)
Ridge.Alpha=0
Ridge.Fit = glmnet(X_train, y_train, alpha=Ridge.Alpha, lambda=grid)
set.seed(2)
bs = sample(dim(train)[1],dim(train)[1],replace = T)
train = train[bs,]
X_train = model.matrix(SalePrice~.,data = train)[,-1]
X_test = model.matrix(SalePrice~.,test)[,-1]
y_train = train$SalePrice
y_test = test$SalePrice
set.seed(1234)
Ridge.Alpha=0
Ridge.Fit = glmnet(X_train, y_train, alpha=Ridge.Alpha, lambda=grid)
set.seed(2)
bs = sample(dim(train)[1],dim(train)[1],replace = T)
train = train[bs,]
X_train = model.matrix(SalePrice~.,data = train)[,-1]
X_test = model.matrix(SalePrice~.,test)[,-1]
y_train = train$SalePrice
y_test = test$SalePrice
set.seed(1234)
Ridge.Alpha=0
Ridge.Fit = glmnet(X_train, y_train, alpha=Ridge.Alpha, lambda=grid)
test = read.csv("./SourceData/test_p3.csv")
y_test = test$SalePrice
x_test = subset (test, select = -SalePrice)
train = read.csv("./SourceData/train_p3.csv")
y_train = train$SalePrice
x_train = subset (train, select = -SalePrice)
y_train = as.numeric(y_train)
y_test = as.numeric(y_test)
summary(y_train)
set.seed(1234)
Ridge.Alpha=0
Ridge.Fit = glmnet(X_train, y_train, alpha=Ridge.Alpha, lambda=grid)
library(e1071)
svmfit=svm(SalePrice~., data=train_cl , kernel="linear", cost=10,
scale=FALSE)
svmfit
library(e1071)
svmfit=svm(SalePrice~., data=train_cl , kernel="linear", cost=10,
scale=FALSE)
svmfit
plot(svmfit, train_cl)
library(e1071)
svmfit=svm(SalePrice~., data=train_cl , kernel="linear", cost=10, scale=FALSE)
svmfit
plot(svmfit, train_cl)
library(e1071)
svmfit=svm(SalePrice~., data=train_cl , kernel="linear", cost=10, scale=FALSE)
svmfit
plot(svmfit, train_cl)
plot(rnorm(50), rnorm(50))
library(e1071)
svmfit=svm(SalePrice~., data=train_cl , kernel="linear", cost=10, scale=FALSE)
svmfit
plot(svmfit, train_cl$SalePrice)
library(e1071)
svmfit=svm(SalePrice~., data=train_cl , kernel="linear", cost=10, scale=FALSE)
svmfit
table(z[final.train], predict(svm.linear, data.train))
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(fig.pos = "H", out.extra = "")
library(pander)
library(knitr)
library(skimr)
library(kableExtra)
library(tinytex)
library(dplyr)
library(purrr)
library(randomForest)
local({
hook_inline = knitr::knit_hooks$get('inline')
knitr::knit_hooks$set(inline = function(x) {
res = hook_inline(x)
if (is.numeric(x)) sprintf('$%s$', res) else res
})
})
set.seed(315)
z = rep(0, 100)
z[train] = 1
set.seed(131)
p = rnorm(100)
gini = p * (1 - p) * 2 +p^4+ rnorm(100)
train = sample(100, 50)
gini[train] = gini[train] + 3
gini[-train] = gini[-train] - 3
plot(p[train], gini[train], pch="1", lwd=4, col="black", ylim=c(-4, 20), xlab="X", ylab="Y")
points(p[-train], gini[-train], pch="2", lwd=4, col="blue")
set.seed(315)
z = rep(0, 100)
z[train] = 1
# Take 25 observations each from train and -train
final.train = c(sample(train, 25), sample(setdiff(1:100, train), 25))
data.train = data.frame(x=p[final.train], y=gini[final.train], z=as.factor(z[final.train]))
data.train
data.test = data.frame(x=p[-final.train], y=gini[-final.train], z=as.factor(z[-final.train]))
library(e1071)
svm.linear = svm(z~., data=data.train, kernel="linear", cost=10)
plot(svm.linear, data.train)
library(e1071)
svmfit=svm(SalePrice~., data=train_cl , kernel="linear", cost=10, scale=FALSE)
svmfit
table(y_train, predict(svmfit, train_cl))
library(e1071)
svmfit=svm(SalePrice~., data=train_cl , kernel="linear", cost=10, scale=FALSE)
svmfit
tune.out=tune(svm,SalePrice~., data=train_cl,kernel="linear",
ranges=list(cost=c(1,5,10) ))
summary(tune.out)
